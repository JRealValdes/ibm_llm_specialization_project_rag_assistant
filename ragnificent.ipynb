{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49fd810e",
   "metadata": {},
   "source": [
    "# RAGnificent\n",
    "A Magnificent RAG for the IBM Specialization \"Generative AI Engineering with LLMs\" final project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e18647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import (\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    JSONLoader,\n",
    "    WebBaseLoader,\n",
    "    TextLoader\n",
    ")\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f9e6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used:\n",
    "facebook_chat_json_path = Path(\"documents\") / \"json\" / \"facebook_chat.json\"\n",
    "markdown_sample_path = 'documents\\markdown\\markdown-sample.md'\n",
    "lora_paper_pdf_path = 'documents\\pdf\\LoRA_paper.pdf'\n",
    "langchain_url = 'https://www.ibm.com/topics/langchain'\n",
    "new_policies_txt_path = Path(\"documents\") / \"txt\" / \"new_policies.txt\"\n",
    "\n",
    "# Unused:\n",
    "# mlb_teams_csv_path = 'documents\\csv\\mlb_teams_2012.csv'\n",
    "# large_scale_alignment_pdf_path = 'documents\\pdf\\large_scale_alignment.pdf'\n",
    "\n",
    "llm_model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "embedding_model_id = 'sentence-transformers/all-mpnet-base-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d82112",
   "metadata": {},
   "source": [
    "## Task 1 - Load document using LangChain for different sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0dd80",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad85fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Review of Low-Rank\n",
      "Adaptation in Large Language Models for\n",
      "Efficient Parameter Tuning\n",
      "September 10, 2024\n",
      "Abstract\n",
      "Natural Language Processing (NLP) often involves pre-training large\n",
      "models on extensive datasets and then adapting them for specific tasks\n",
      "through fine-tuning. However, as these models grow larger, like GPT-3\n",
      "with 175 billion parameters, fully fine-tuning them becomes computa-\n",
      "tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
      "Adaptation) that significantly reduces the overhead by freezing the orig-\n",
      "inal model weights and only training small rank decomposition matrices.\n",
      "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
      "GPU memory usage by three times. LoRA not only maintains but some-\n",
      "times surpasses fine-tuning performance on models like RoBERTa, De-\n",
      "BERTa, GPT-2, and GPT-3.\n",
      "Unlike other methods, LoRA introduces\n",
      "no extra latency during inference, making it more efficient for practical\n",
      "applications.\n",
      "All relevant code and model checkpoints are available at\n",
      "https://github.com/microsoft/LoRA.\n",
      "1\n",
      "Introduction\n",
      "Many natural language processing (NLP) applications rely on adapting large,\n",
      "pre-trained language models for various downstream tasks. Typically, this is\n",
      "done through fine-tuning, where all the parameters of the pre-trained model are\n",
      "updated. However, a significant drawback of fine-tuning is that the adapted\n",
      "model has just as many parameters as the original one. As models grow in size,\n",
      "what was once a manageable issue for models like GPT-2 or RoBERTa large\n",
      "becomes a serious deployment challenge with larger models like GPT-3, which\n",
      "has 175 billion trainable parameters.\n",
      "To mitigate these challenges, researchers have explored adapting only cer-\n",
      "tain parts of the model or adding external modules specific to each task. This\n",
      "approach reduces the need to store and manage large numbers of parameters\n",
      "for each task, greatly improving efficiency during deployment. However, cur-\n",
      "rent methods often introduce drawbacks, such as inference delays by increasing\n",
      "1\n",
      "model depth or reducing the usable sequence length. Furthermore, these meth-\n",
      "ods typically do not perform as well as full fine-tuning, leading to a trade-off\n",
      "between efficiency and model performance.\n",
      "Inspired by prior works that demonstrate over-parametrized models often\n",
      "reside in a low intrinsic dimensional space, we hypothesize that weight changes\n",
      "during model adaptation also have a low “intrinsic rank.” This insight leads to\n",
      "our Low-Rank Adaptation (LoRA) approach. LoRA optimizes low-rank decom-\n",
      "position matrices for the dense layers’ weight changes during adaptation, while\n",
      "keeping the pre-trained weights frozen. As illustrated in Figure 1, even with\n",
      "large models like GPT-3 (with up to 12,288 dimensions in full rank), a low-rank\n",
      "matrix (rank 1 or 2) is sufficient, making LoRA highly efficient in terms of both\n",
      "storage and computation.\n",
      "LoRA has several notable advantages:\n",
      "• The pre-trained model can be shared, and small LoRA modules can be\n",
      "created for various tasks. By freezing the main model and only switching\n",
      "the matrices A and B (shown in Figure 1), storage and task-switching\n",
      "overhead are significantly reduced.\n",
      "• LoRA improves training efficiency and reduces hardware requirements,\n",
      "lowering the entry barrier by up to threefold when using adaptive opti-\n",
      "mizers. This is because LoRA only requires updating the smaller low-rank\n",
      "matrices, avoiding the need to calculate gradients for most parameters.\n",
      "• The simple linear design allows merging of the trainable matrices with\n",
      "the frozen pre-trained weights during deployment, ensuring no additional\n",
      "inference latency compared to fully fine-tuned models.\n",
      "• LoRA is compatible with many existing methods and can be combined\n",
      "with approaches like prefix-tuning.\n",
      "In this work, we follow standard conventions for Transformer architecture\n",
      "and refer to dimensions such as dmodel, and projection matrices like Wq, Wk, Wv,\n",
      "and Wo for the self-attention module. W or W0 represents a pre-trained weight\n",
      "matrix, while ∆W refers to its update during adaptation. The rank r denotes\n",
      "the rank of a LoRA module. Throughout, we use Adam for optimization and\n",
      "maintain the Transformer MLP feedforward dimension as dffn = 4 × dmodel.\n",
      "1.1\n",
      "Key Advantages of LoRA\n",
      "• Efficient Task Switching: A pre-trained model can support multiple\n",
      "tasks by swapping the small LoRA matrices, reducing storage needs.\n",
      "• Reduced Hardware Requirements: LoRA lowers the GPU memory\n",
      "needed for training by freezing most parameters and only training the\n",
      "low-rank matrices.\n",
      "• No Additional Latency: LoRA incurs no extra inference delay because\n",
      "the matrices can be merged with the pre-trained weights when deployed.\n",
      "2\n",
      "• Combining with Other Methods: LoRA can be used with other ap-\n",
      "proaches, like prefix-tuning, to further optimize model performance.\n",
      "2\n",
      "Problem Statement\n",
      "Although our approach is independent of the specific training objective, we focus\n",
      "on language modeling as the central application.\n",
      "Below, we outline the key\n",
      "aspects of the language modeling problem, particularly the goal of maximizing\n",
      "conditional probabilities based on task-specific prompts.\n",
      "Assume we have an autoregressive language model PΦ(y|x) that is pre-\n",
      "trained and parameterized by Φ.\n",
      "For example, PΦ(y|x) could be a general\n",
      "multi-task model such as GPT, built on top of the Transformer architecture.\n",
      "The model can then be adapted to different downstream tasks such as text\n",
      "summarization, machine reading comprehension (MRC), and natural language\n",
      "to SQL (NL2SQL). Each downstream task is represented as a training set of\n",
      "context-output pairs:\n",
      "Z = {(xi, yi)}i=1,...,N,\n",
      "where both xi and yi are sequences of tokens. For instance, in NL2SQL, xi\n",
      "might represent a natural language question and yi would be the corresponding\n",
      "SQL query; in summarization, xi represents the article and yi would be its\n",
      "summary.\n",
      "In traditional fine-tuning, the model is initialized using the pre-trained weights\n",
      "Φ0, which are then updated to Φ0 + ∆Φ by optimizing the model’s parameters\n",
      "to maximize the conditional probabilities for each token:\n",
      "max\n",
      "Φ\n",
      "X\n",
      "(x,y)∈Z\n",
      "|y|\n",
      "X\n",
      "t=1\n",
      "log (PΦ(yt|x, y<t))\n",
      "(1)\n",
      "A significant limitation of full fine-tuning is that for every downstream task, a\n",
      "different set of parameters ∆Φ must be learned, and the size of ∆Φ is equal to the\n",
      "size of Φ0. For large models, such as GPT-3 with 175 billion parameters, storing\n",
      "and deploying multiple instances of fine-tuned models becomes impractical or\n",
      "extremely challenging.\n",
      "To address this issue, we propose a more efficient approach where the task-\n",
      "specific parameter updates ∆Φ = ∆Φ(Θ) are encoded using a much smaller set\n",
      "of parameters Θ, where |Θ| ≪|Φ0|. As a result, optimizing the model for each\n",
      "task reduces to optimizing Θ as follows:\n",
      "max\n",
      "Θ\n",
      "X\n",
      "(x,y)∈Z\n",
      "|y|\n",
      "X\n",
      "t=1\n",
      "log\n",
      "\u0000pΦ0+∆Φ(Θ)(yt|x, y<t)\n",
      "\u0001\n",
      "(2)\n",
      "In the following sections, we explore a low-rank approach for representing\n",
      "∆Φ, making the adaptation process more efficient in both computational and\n",
      "memory terms.\n",
      "For large models like GPT-3 175B, this method allows the\n",
      "trainable parameters |Θ| to be reduced to as little as 0.01% of |Φ0|.\n",
      "3\n",
      "3\n",
      "Limitations on Current Solutions\n",
      "The challenge we aim to address is not new. Since the rise of transfer learning,\n",
      "a great deal of work has focused on making model adaptation more efficient in\n",
      "terms of both parameters and computation. For an overview, see Section 6 for\n",
      "some well-known works. Focusing on language modeling, two prominent strate-\n",
      "gies for efficient adaptation stand out: adding adapter layers, or optimizing the\n",
      "input layer activations. However, both approaches come with limitations, espe-\n",
      "cially when applied in large-scale, latency-sensitive production environments.\n",
      "3.1\n",
      "Adapter Layers and Inference Latency\n",
      "There are many variations of adapters. We focus on the original adapter design\n",
      "from [?], which introduces two adapter layers per Transformer block, and a\n",
      "more recent approach by [?], which only uses one adapter per block but with an\n",
      "additional LayerNorm [?]. Although overall latency can be reduced by pruning\n",
      "layers or leveraging multi-task settings [?], [?], there is no way to completely\n",
      "eliminate the additional computation introduced by adapter layers. This might\n",
      "seem minor since adapters generally have few parameters (typically less than\n",
      "1% of the original model) due to their small bottleneck dimension, which limits\n",
      "the number of floating-point operations (FLOPs). However, large-scale neural\n",
      "networks rely heavily on parallel processing to maintain low latency, and adapter\n",
      "layers are processed sequentially. This becomes more evident in scenarios with\n",
      "low batch sizes, such as real-time inference, where models like GPT-2 [?] running\n",
      "on a single GPU experience noticeable increases in latency, even with small\n",
      "bottleneck dimensions (Table 1).\n",
      "The issue is further compounded when models need to be sharded across\n",
      "multiple devices, since the increased model depth requires more synchronous\n",
      "GPU operations like AllReduce and Broadcast, unless adapter parameters are\n",
      "redundantly replicated.\n",
      "3.2\n",
      "Challenges with Directly Optimizing the Prompt\n",
      "Another approach, such as prefix tuning [?], faces a different challenge. We\n",
      "have observed that prefix tuning is often difficult to optimize and that its per-\n",
      "formance does not consistently improve as more trainable parameters are added,\n",
      "confirming earlier findings. Moreover, allocating part of the sequence length for\n",
      "adaptation inevitably reduces the available sequence length for processing task-\n",
      "related data, which seems to hinder prompt tuning’s performance compared to\n",
      "other methods. We will further explore this issue in Section 5.\n",
      "4\n",
      "Batch Size\n",
      "Sequence Length\n",
      "|Θ|\n",
      "Latency (ms)\n",
      "Fine-Tune/LoRA\n",
      "32\n",
      "512\n",
      "0.5M\n",
      "1449.4 ± 0.8\n",
      "16\n",
      "256\n",
      "11M\n",
      "338.0 ± 0.6\n",
      "1\n",
      "128\n",
      "11M\n",
      "19.8 ± 2.7\n",
      "AdapterL\n",
      "32\n",
      "512\n",
      "0.5M\n",
      "1482.0 ± 1.0 (+2.2%)\n",
      "16\n",
      "256\n",
      "11M\n",
      "354.8 ± 0.5 (+5.0%)\n",
      "1\n",
      "128\n",
      "11M\n",
      "23.9 ± 2.1 (+20.7%)\n",
      "AdapterH\n",
      "32\n",
      "512\n",
      "0.5M\n",
      "1492.2 ± 1.0 (+3.0%)\n",
      "16\n",
      "256\n",
      "11M\n",
      "366.3 ± 0.5 (+8.4%)\n",
      "1\n",
      "128\n",
      "11M\n",
      "25.8 ± 2.2 (+30.3%)\n",
      "Table 1: Inference latency of a forward pass in GPT-2 Medium measured over\n",
      "100 trials using an NVIDIA Quadro RTX8000. ”|Θ|” refers to the number of\n",
      "trainable parameters in the adapter layers. AdapterL and AdapterH are two\n",
      "types of adapter tuning. The impact on latency becomes significant, particularly\n",
      "in online scenarios with shorter sequences and smaller batch sizes.\n",
      "4\n",
      "Our Method\n",
      "In this section, we explain the structure of LoRA and its practical benefits.\n",
      "The principles outlined here apply generally to dense layers in neural networks,\n",
      "although we focus on specific weights in Transformer language models, as these\n",
      "models serve as the central example in our experiments.\n",
      "4.1\n",
      "Low-Rank Parameterized Update Matrices\n",
      "Neural networks contain numerous dense layers that perform matrix multipli-\n",
      "cation, and the weight matrices in these layers typically have a full rank. When\n",
      "adapting to a particular task, it shows that pre-trained language models pos-\n",
      "sess a low ”intrinsic dimension” and can still perform effectively after a random\n",
      "projection to a smaller subspace. Drawing inspiration from this, we hypothesize\n",
      "that updates to the weights during adaptation also have a low ”intrinsic rank.”\n",
      "For a pre-trained weight matrix W0 ∈Rd×k, we limit its update by express-\n",
      "ing it as a low-rank decomposition, W0 + ∆W = W0 + BA, where B ∈Rd×r,\n",
      "A ∈Rr×k, and the rank r ≪min(d, k). During training, W0 is fixed, and A\n",
      "and B are the trainable parameters. Both W0 and ∆W = BA are multiplied\n",
      "with the input, and their respective outputs are summed element-wise. Thus,\n",
      "for h = W0x, our updated forward pass becomes:\n",
      "h = W0x + ∆Wx = W0x + BAx\n",
      "We illustrate this reparametrization in Figure 1. We initialize A with random\n",
      "Gaussian values and set B to zero, meaning ∆W = BA is zero at the start\n",
      "5\n",
      "of training. We then scale ∆Wx by α\n",
      "r , where α is a constant dependent on\n",
      "r.\n",
      "When using Adam for optimization, adjusting α has an effect similar to\n",
      "tuning the learning rate. Therefore, we use the same α for our first experiments\n",
      "and avoid tuning it. This scaling method also minimizes the need to adjust\n",
      "hyperparameters when varying r.\n",
      "4.1.1\n",
      "A Generalization of Full Fine-Tuning\n",
      "A more general fine-tuning technique involves training only a subset of pre-\n",
      "trained parameters. LoRA extends this approach by eliminating the need for\n",
      "full-rank gradient updates to weight matrices. Instead, LoRA uses low-rank\n",
      "matrices for adaptation.\n",
      "If LoRA is applied to all weight matrices, and all\n",
      "biases are trained, the expressiveness of full fine-tuning is recovered by setting\n",
      "the LoRA rank r equal to the rank of the pre-trained weight matrices. As the\n",
      "number of trainable parameters increases, LoRA approaches the full fine-tuning\n",
      "performance, while adapter-based techniques converge to simpler models that\n",
      "cannot process long input sequences.\n",
      "4.1.2\n",
      "No Additional Inference Latency\n",
      "When deploying LoRA, we can explicitly compute W = W0 + BA and use it\n",
      "during inference. This means that when switching between tasks, we can quickly\n",
      "subtract BA and add a different low-rank matrix B′A′ without consuming ex-\n",
      "tra memory. This ensures that no additional inference latency is introduced\n",
      "compared to fully fine-tuned models.\n",
      "4.2\n",
      "Applying LoRA to Transformer Models\n",
      "In principle, LoRA can be applied to any subset of weight matrices in a neural\n",
      "network to minimize the number of trainable parameters.\n",
      "In a Transformer\n",
      "architecture, the self-attention module contains four projection matrices Wq,\n",
      "Wk, Wv, and Wo, and the MLP module contains two more matrices. We treat\n",
      "the weight matrices in the self-attention module as a single dmodel × dmodel\n",
      "matrix, despite them being split into different attention heads.\n",
      "To simplify\n",
      "the process and improve parameter efficiency, we restrict our method to only\n",
      "adapting the attention weights for downstream tasks, leaving the MLP module\n",
      "frozen. The effect of adapting various attention weight matrices in a Transformer\n",
      "is further explored in Section 7.1. We leave the investigation of adapting MLP\n",
      "layers, LayerNorm layers, and biases to future work.\n",
      "4.2.1\n",
      "Practical Benefits and Limitations\n",
      "One of the major advantages of LoRA is its reduction in memory and storage\n",
      "costs. For large Transformer models using Adam, LoRA can cut VRAM usage\n",
      "by up to two-thirds if r ≪dmodel, since it eliminates the need to store optimizer\n",
      "states for frozen parameters. For example, with GPT-3 175B, VRAM usage\n",
      "during training drops from 1.2 TB to 350 GB. With r = 4, and only the query\n",
      "6\n",
      "and value matrices being adapted, the checkpoint size decreases by approxi-\n",
      "mately 10,000× (from 350 GB to 35 MB). This makes it possible to train using\n",
      "significantly fewer GPUs and avoid I/O bottlenecks. LoRA also enables easier\n",
      "task-switching during deployment by simply swapping out the LoRA weights,\n",
      "which requires far less memory than loading entirely new model parameters.\n",
      "Additionally, LoRA offers a 25% training speedup compared to full fine-tuning\n",
      "because there is no need to compute gradients for most parameters.\n",
      "However, LoRA does have some limitations.\n",
      "It is not straightforward to\n",
      "combine multiple tasks with different low-rank matrices A and B in a single\n",
      "forward pass if BA is absorbed into W to remove additional inference latency.\n",
      "While it is possible to dynamically select LoRA modules during inference, this\n",
      "solution is not suitable for scenarios where low-latency responses are crucial.\n",
      "5\n",
      "Empirical Experiments\n",
      "We assess LoRA’s performance in downstream tasks across several models in-\n",
      "cluding RoBERTa, DeBERTa, and GPT-2, before scaling up to GPT-3 175B.\n",
      "Our experiments cover various tasks, ranging from natural language understand-\n",
      "ing (NLU) to natural language generation (NLG). For RoBERTa and DeBERTa,\n",
      "we evaluate on the GLUE benchmark. All experiments were performed using\n",
      "NVIDIA Tesla V100 GPUs.\n",
      "5.1\n",
      "Baselines\n",
      "For comparison with a wide range of baselines, we replicate experimental setups\n",
      "from previous studies and, where possible, reuse reported results. This might\n",
      "result in some baselines being present in only a subset of experiments.\n",
      "Fine-Tuning (FT) is a common method for adapting models. During fine-\n",
      "tuning, the model’s pre-trained weights and biases are updated using gradient\n",
      "descent. A variant of this is fine-tuning only select layers, while freezing the\n",
      "rest. One such baseline from prior work on GPT-2 updates only the last two\n",
      "layers (denoted as FTTop2).\n",
      "BitFit is another baseline in which only the bias parameters are updated,\n",
      "while all other parameters remain frozen. This method has gained attention,\n",
      "including in recent studies [?].\n",
      "Prefix-embedding tuning (PreEmbed) involves adding special tokens to\n",
      "the input sequence, and training their embeddings. These tokens do not belong\n",
      "to the model’s original vocabulary. Their placement—either prepended (prefix)\n",
      "or appended (infix)—can significantly affect performance, as highlighted in [?].\n",
      "Prefix-layer tuning (PreLayer) extends prefix tuning by learning train-\n",
      "able activations at each Transformer layer. This results in a larger number of\n",
      "trainable parameters, as activations from prior layers are progressively replaced.\n",
      "The total number of trainable parameters is given by |Θ| = L×dmodel ×(lp +li),\n",
      "where L is the number of Transformer layers.\n",
      "7\n",
      "Adapter tuning [?] introduces additional fully connected adapter layers\n",
      "between existing layers in the Transformer.\n",
      "Several variants exist, such as\n",
      "AdapterH and AdapterL [?], which differ in the placement of adapters within\n",
      "the network. The number of trainable parameters in these methods is |Θ| =\n",
      "LAdpt × (2 × dmodel × r + r + dmodel) + 2 × LLN × dmodel.\n",
      "LoRA, on the other hand, introduces trainable low-rank matrices to the ex-\n",
      "isting weight matrices. As detailed in Section 4.2, LoRA is applied to the query\n",
      "and value matrices in most experiments. The number of trainable parameters\n",
      "is determined by the rank r and the shape of the original weight matrices:\n",
      "|Θ| = 2 × LLoRA × dmodel × r, where LLoRA represents the number of weight\n",
      "matrices to which LoRA is applied.\n",
      "Table 2: GPT-2 Medium and Large results on E2E NLG Challenge. Higher\n",
      "scores are better for all metrics. Confidence intervals are provided for experi-\n",
      "ments we conducted. *Results from prior work.\n",
      "Model & Method\n",
      "# Trainable Parameters\n",
      "BLEU\n",
      "NIST\n",
      "MET\n",
      "ROUGE-L\n",
      "CIDEr\n",
      "GPT-2 M (FT)*\n",
      "354.92M\n",
      "68.2\n",
      "8.62\n",
      "46.2\n",
      "71.0\n",
      "2.47\n",
      "GPT-2 M (LoRA)\n",
      "0.35M\n",
      "70.4 ± 0.1\n",
      "8.85 ± 0.2\n",
      "46.8 ± 0.2\n",
      "71.8 ± 0.1\n",
      "2.53 ± 0.2\n",
      "GPT-2 L (FT)*\n",
      "774.03M\n",
      "68.5\n",
      "8.78\n",
      "46.0\n",
      "69.9\n",
      "2.45\n",
      "GPT-2 L (LoRA)\n",
      "0.77M\n",
      "70.4 ± 0.1\n",
      "8.89 ± 0.2\n",
      "46.8 ± 0.2\n",
      "72.0 ± 0.2\n",
      "2.47 ± 0.2\n",
      "5.2\n",
      "Scaling LoRA to GPT-3 175B\n",
      "To further test LoRA’s scalability, we apply it to GPT-3 175B. Given the large\n",
      "computational cost of GPT-3, we only report standard deviations for each task\n",
      "based on multiple random seeds. See Appendix D.4 for hyperparameters used.\n",
      "As presented in Table ??, LoRA matches or outperforms full fine-tuning on\n",
      "WikiSQL, MultiNLI, and SAMSum. Notably, we observe that certain methods\n",
      "do not consistently benefit from increasing the number of trainable parame-\n",
      "ters. As shown in Figure 1, LoRA remains efficient even at low ranks, avoiding\n",
      "the performance degradation seen with larger token embeddings in prefix-based\n",
      "methods.\n",
      "Figure 1: GPT-3 175B validation accuracy vs. the number of trainable parame-\n",
      "ters for several adaptation methods on WikiSQL and MNLI. LoRA demonstrates\n",
      "better scalability and performance.\n",
      "8\n",
      "6\n",
      "Related Works\n",
      "6.1\n",
      "Transformer Language Models\n",
      "The Transformer architecture, as introduced by Vaswani et al.\n",
      "(2017), has\n",
      "proven to be a highly effective sequence-to-sequence model due to its heavy use\n",
      "of self-attention mechanisms. Radford et al. (2018) applied it to autoregressive\n",
      "language modeling, significantly boosting its utility in the field.\n",
      "Since then,\n",
      "Transformer-based models have become a staple in natural language processing\n",
      "(NLP), achieving state-of-the-art results in a wide variety of tasks. Notably,\n",
      "BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have paved the\n",
      "way for large-scale pre-trained language models that, when fine-tuned, deliver\n",
      "excellent performance on specific tasks. The next breakthrough came with GPT-\n",
      "3 (Brown et al., 2020), which is currently the largest single Transformer language\n",
      "model with 175 billion parameters.\n",
      "6.2\n",
      "Prompt Engineering and Fine-Tuning\n",
      "Despite GPT-3’s ability to adapt its behavior with minimal data (few-shot learn-\n",
      "ing), its performance is highly sensitive to how the input prompt is structured\n",
      "(Brown et al., 2020). This has led to the rise of ”prompt engineering,” a process\n",
      "that involves crafting and fine-tuning the input prompts to maximize model per-\n",
      "formance on specific tasks. Fine-tuning, on the other hand, refers to retraining\n",
      "a model pre-trained on general domains to adapt it to a particular task (Devlin\n",
      "et al., 2018; Radford et al., 2018). Some approaches only update a subset of the\n",
      "model’s parameters (Collobert and Weston, 2008), but it is common practice to\n",
      "fine-tune all parameters to achieve the best performance. However, performing\n",
      "full fine-tuning on a model as large as GPT-3, with its 175 billion parameters,\n",
      "poses significant challenges due to the large memory requirements and the com-\n",
      "putational resources needed, making it as resource-intensive as pre-training.\n",
      "6.3\n",
      "Parameter-Efficient Adaptation\n",
      "Many techniques have been developed to address the inefficiency of full fine-\n",
      "tuning by adapting only certain layers or introducing adapter modules. Houlsby\n",
      "et al. (2019), Rebuffi et al. (2017), and Lin et al. (2020) proposed inserting\n",
      "adapter layers between existing layers in the network. These adapters allow for\n",
      "parameter-efficient adaptation by learning only a small number of task-specific\n",
      "parameters. Our method imposes a low-rank constraint on the weight updates,\n",
      "ensuring that learned weights can be merged with the main model weights dur-\n",
      "ing inference, thus introducing no additional latency, unlike the adapter layers.\n",
      "A related approach, COMPACTER (Mahabadi et al., 2021), uses Kronecker\n",
      "products to parametrize the adapters, further improving parameter efficiency.\n",
      "Additionally, prompt optimization techniques, such as those proposed by Li and\n",
      "Liang (2021), Lester et al. (2021), and Hambardzumyan et al. (2020), aim to\n",
      "optimize the input tokens directly. However, these approaches typically reduce\n",
      "9\n",
      "the available sequence length for task processing. Our work can be combined\n",
      "with such methods for further gains in efficiency.\n",
      "6.4\n",
      "Low-Rank Structures in Deep Learning\n",
      "Low-rank structures are prevalent in many machine learning problems, and sev-\n",
      "eral studies have explored imposing these constraints on deep models. Li et\n",
      "al. (2016), Cai et al. (2010), and Grasedyck et al. (2013) showed that many\n",
      "learning tasks have an intrinsic low-rank structure. For deep neural networks,\n",
      "particularly over-parameterized models, it has been shown that they often ex-\n",
      "hibit low-rank properties after training (Oymak et al., 2019). Prior works, such\n",
      "as those by Sainath et al. (2013), Zhang et al. (2014), and Denil et al. (2014),\n",
      "have explicitly imposed low-rank constraints during training to enhance model\n",
      "efficiency. However, our approach differs in that we apply low-rank updates to\n",
      "frozen pre-trained models, making it highly effective for task-specific adapta-\n",
      "tion. Neural networks with low-rank structures have been shown to outperform\n",
      "classical methods such as finite-width neural tangent kernels (Allen-Zhu et al.,\n",
      "2019; Li and Liang, 2018), and low-rank adaptations are particularly useful in\n",
      "adversarial training scenarios (Allen-Zhu and Li, 2020). This makes our pro-\n",
      "posed low-rank adaptation well-grounded in both theory and practice.\n",
      "7\n",
      "Analyzing Low-Rank Adaptations\n",
      "In light of the demonstrated benefits of LoRA, we aim to further explore the\n",
      "attributes of low-rank adaptation as applied to various downstream tasks. The\n",
      "low-rank structure does not only reduce the hardware requirements for conduct-\n",
      "ing parallel experiments, but it also provides better insight into how adapted\n",
      "weights align with pre-trained weights. Our focus lies on GPT-3 175B, where\n",
      "we managed to significantly reduce the number of trainable parameters (up to\n",
      "10,000×) without sacrificing task performance.\n",
      "In this section, we address some key questions:\n",
      "• 1) With a constrained parameter budget, which weight matrices should\n",
      "be adapted to achieve the best downstream task performance?\n",
      "• 2) Is the adapted matrix ∆W truly rank-deficient, and if so, what rank is\n",
      "optimal for practical use?\n",
      "• 3) How is ∆W related to the pre-trained weights W? Does ∆W exhibit\n",
      "high correlation with W, and what is the comparative size of ∆W to W?\n",
      "The answers to these questions provide valuable insights for optimizing pre-\n",
      "trained models for downstream tasks.\n",
      "7.1\n",
      "Selecting Optimal Weight Matrices for LoRA\n",
      "To optimize performance under a limited parameter budget, we explore adapting\n",
      "different weight matrices within the self-attention module of the Transformer.\n",
      "10\n",
      "We allocate 18M parameters (approximately 35MB stored in FP16) for GPT-3\n",
      "175B, using a rank r = 8 for one attention weight type or r = 4 for two types.\n",
      "The results are displayed in Table 3.\n",
      "Table 3: Validation accuracy on WikiSQL and MultiNLI with LoRA applied to\n",
      "different attention weights in GPT-3, with a fixed number of trainable parame-\n",
      "ters.\n",
      "# of Trainable Parameters = 18M\n",
      "Wq\n",
      "Wk\n",
      "Wv\n",
      "Wo\n",
      "Wq, Wv\n",
      "Rank r = 8\n",
      "70.4\n",
      "70.0\n",
      "73.0\n",
      "73.2\n",
      "73.7\n",
      "MultiNLI (±0.1%)\n",
      "91.0\n",
      "90.8\n",
      "91.0\n",
      "91.3\n",
      "91.7\n",
      "7.2\n",
      "Determining the Ideal Rank for LoRA\n",
      "To analyze the impact of the rank r on task performance, we applied LoRA with\n",
      "varying ranks across different combinations of attention matrices. The results\n",
      "can be found in Table 4.\n",
      "Table 4: Validation accuracy on WikiSQL and MultiNLI with different ranks r.\n",
      "Weight Type\n",
      "r = 1\n",
      "r = 2\n",
      "r = 4\n",
      "r = 8\n",
      "r = 64\n",
      "WikiSQL (±0.5%) Wq\n",
      "68.8\n",
      "69.6\n",
      "70.5\n",
      "70.4\n",
      "70.0\n",
      "WikiSQL (Wq, Wv)\n",
      "73.4\n",
      "73.3\n",
      "73.7\n",
      "73.8\n",
      "73.5\n",
      "MultiNLI (±0.1%) Wq\n",
      "90.7\n",
      "90.9\n",
      "91.1\n",
      "90.7\n",
      "90.7\n",
      "MultiNLI (Wq, Wv)\n",
      "91.3\n",
      "91.4\n",
      "91.3\n",
      "91.6\n",
      "91.4\n",
      "The results show that even at a small rank r = 1, LoRA performs well when\n",
      "both Wq and Wv are adapted. In contrast, adapting only Wq requires a higher\n",
      "rank for optimal performance.\n",
      "8\n",
      "Conclusion\n",
      "LoRA offers a highly efficient solution to the problem of adapting large language\n",
      "models for downstream tasks. By freezing the majority of the model’s param-\n",
      "eters and training only small, low-rank matrices, LoRA achieves comparable\n",
      "performance to full fine-tuning while drastically reducing computational costs.\n",
      "Its ability to scale to massive models like GPT-3 without sacrificing performance\n",
      "highlights its potential for widespread use.\n",
      "Future work could explore combining LoRA with other parameter-efficient\n",
      "methods or investigating more principled ways to select which weight matrices\n",
      "to adapt. Additionally, further studies on the rank deficiency of pre-trained\n",
      "weights could inspire new developments in efficient model adaptation.\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyMuPDFLoader(lora_paper_pdf_path)\n",
    "pdf_data = pdf_loader.load()\n",
    "pdf_text_content = '\\n'.join([page.page_content for page in pdf_data])\n",
    "print(pdf_text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2a0f4",
   "metadata": {},
   "source": [
    "### Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a15a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_loader = UnstructuredMarkdownLoader(markdown_sample_path)\n",
    "md_data = md_loader.load()\n",
    "# print(md_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7762e",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_loader = JSONLoader(\n",
    "    file_path=facebook_chat_json_path,\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "json_data = json_loader.load()\n",
    "# print(json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b259fa3",
   "metadata": {},
   "source": [
    "### Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce30d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_loader = WebBaseLoader(langchain_url)\n",
    "web_data = web_loader.load()\n",
    "# print(web_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e432b5",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_loader = TextLoader(new_policies_txt_path)\n",
    "txt_data = txt_loader.load()\n",
    "# print(txt_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ff9af",
   "metadata": {},
   "source": [
    "## Task 2 - Apply text splitting techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f565db6",
   "metadata": {},
   "source": [
    "### Recursive Character Text Splitter - On PDF file content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af58c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created from PDF: 73\n",
      "First two chunks' content:\n",
      "['A Comprehensive Review of Low-Rank\\nAdaptation in Large Language Models for\\nEfficient Parameter Tuning\\nSeptember 10, 2024\\nAbstract\\nNatural Language Processing (NLP) often involves pre-training large\\nmodels on extensive datasets and then adapting them for specific tasks\\nthrough fine-tuning. However, as these models grow larger, like GPT-3', 'with 175 billion parameters, fully fine-tuning them becomes computa-\\ntionally expensive. We propose a novel method called LoRA (Low-Rank\\nAdaptation) that significantly reduces the overhead by freezing the orig-\\ninal model weights and only training small rank decomposition matrices.\\nThis leads to up to 10,000 times fewer trainable parameters and reduces']\n"
     ]
    }
   ],
   "source": [
    "rc_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks = rc_text_splitter.split_text(pdf_text_content)\n",
    "print(f\"Number of chunks created from PDF: {len(chunks)}\")\n",
    "print(f\"First two chunks' content:\\n{chunks[:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2f256",
   "metadata": {},
   "source": [
    "### Code Splitter on Python code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b434cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "    def hello_world():\n",
    "        print(\"Hello, World!\")\n",
    "    \n",
    "    # Call the function\n",
    "    hello_world()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbbc0c",
   "metadata": {},
   "source": [
    "## Task 3 - Embed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5549f15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jreal\\AppData\\Local\\Temp\\ipykernel_32708\\1541313057.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  huggingface_embedding = HuggingFaceEmbeddings(model_name=embedding_model_id)\n",
      "c:\\Users\\jreal\\Documents\\Stark Industries\\GitHub\\ibm_llm_specialization_project_rag_assistant\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "huggingface_embedding = HuggingFaceEmbeddings(model_name=embedding_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4ada38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 embeddings for the chunks:\n",
      "[[0.035864997655153275, 0.05806204676628113, -0.019342534244060516, 0.028478598222136497, 0.010876252315938473, 0.03260980173945427, -0.01797351986169815, 0.028705885633826256, -0.007086438592523336, -0.0628940761089325, -0.025137027725577354, -0.010378682985901833, -0.012204059399664402, 0.003963047172874212, 0.033678196370601654, -0.05547269061207771, 0.07979834079742432, -0.021819014102220535, -0.004911839962005615, -0.01717427186667919, -0.031394582241773605, 0.007547338958829641, -0.0020945644937455654, 0.018018919974565506, 0.057778891175985336, 0.0013638543896377087, 0.0023492504842579365, 0.0010475687449797988, 0.001231522997841239, 0.009324378333985806, -0.006343798246234655, 0.06647956371307373, 0.030796635895967484, 0.052936799824237823, 1.7924422763826442e-06, -0.04325321316719055, -0.04193982109427452, -0.009559628553688526, -0.014006862416863441, 0.021783683449029922, 0.020374689251184464, -0.01465480774641037, -0.024416940286755562, 0.024526527151465416, -0.040015604346990585, 0.011730553582310677, 0.06982745230197906, 0.017692122608423233, 0.0374981090426445, 0.0965326577425003, -0.009624820202589035, -0.016039732843637466, 0.0005021327524445951, -0.035337429493665695, -0.010659930296242237, -0.06130335479974747, 0.04098223149776459, -0.03682491183280945, -0.03578483313322067, 0.009298795834183693, -0.048634566366672516, -0.009771331213414669, -0.048851095139980316, 0.028040992096066475, 0.0067565361969172955, 0.008974800817668438, -0.0680314153432846, 0.005747796501964331, -0.018842702731490135, 0.031407639384269714, -0.06439583748579025, -0.020264875143766403, -0.00859422329813242, -0.034689147025346756, 0.013901381753385067, -0.02850470319390297, -0.028494881466031075, -0.039761096239089966, 0.04861992970108986, 0.001678314059972763, 0.020021291449666023, -0.029906267300248146, 0.01871282048523426, 0.0003390951605979353, -0.06055169925093651, 0.05551785230636597, 0.03517002612352371, -0.04806206747889519, -0.002677423181012273, -0.01950160413980484, -0.06475893408060074, 0.007963466458022594, -0.021609416231513023, 0.003510883077979088, -0.015321212820708752, 0.011638645082712173, -0.04004419222474098, -0.03712882846593857, -0.007642448414117098, -0.04453574866056442, 0.0794839933514595, 0.03575904294848442, 0.03849054500460625, 0.03868475556373596, -0.05838623642921448, 0.028509922325611115, -0.01687616854906082, 0.014320174232125282, -0.023867065086960793, 0.012703614309430122, -0.030413759872317314, 0.004254566039890051, -0.04864043369889259, 0.015177099034190178, -0.048089802265167236, 0.0017018034122884274, -0.04990591108798981, 0.024692010134458542, 0.010404741391539574, -0.018773389980196953, 0.02472972683608532, -0.007403821684420109, -0.048128992319107056, -0.02317710407078266, -0.009066409431397915, 0.058627255260944366, 0.03639621660113335, 0.024307409301400185, -0.034514110535383224, -0.010887023992836475, -0.010960089042782784, -0.017207177355885506, 0.05273456498980522, -0.05720016732811928, 0.04214337840676308, 0.04052059352397919, -0.005602514836937189, -0.04829491302371025, -0.01200125552713871, 0.06714893877506256, -0.02975495532155037, -0.06540818512439728, -0.010886918753385544, -0.008805477060377598, 0.008464593440294266, 0.015350568108260632, 0.043791335076093674, -0.009962426498532295, -0.017244242131710052, -0.015817200765013695, -0.09352295845746994, 0.06284065544605255, 0.009002874605357647, 0.0018208998953923583, -0.0005114360828883946, -0.03219938650727272, 0.017728863283991814, 0.06919647008180618, 0.009640632197260857, -0.013491637073457241, 0.03385411947965622, -0.05430155247449875, -0.06547113507986069, 0.045075491070747375, 0.0025359480641782284, 0.013751644641160965, 0.005201342049986124, 0.00825525913387537, 0.01657627522945404, -0.014568171463906765, 0.016984662041068077, 0.013256963342428207, 0.01143014058470726, -0.03289300203323364, 0.07225042581558228, 0.07311847060918808, 0.08380667120218277, 0.06883513927459717, -0.004785533528774977, -0.0318644754588604, -0.005556080956012011, -0.0243317112326622, -0.0241539403796196, -0.00828061904758215, -0.08033644407987595, -0.011403504759073257, 0.0010947101982310414, 0.06833593547344208, 0.0019880509935319424, -0.01538276206701994, -0.02930324710905552, 0.0471804104745388, -0.013967125676572323, -0.009118585847318172, 0.03070969507098198, -0.010057776235044003, -0.02553205005824566, -0.0027333113830536604, 0.022298172116279602, -0.02899976074695587, -0.02703748643398285, -0.07085629552602768, -0.013493025675415993, 0.02504575438797474, 0.006502191070467234, -0.04265822470188141, -0.017809048295021057, 0.0036742188967764378, 0.0009303782717324793, 0.017144182696938515, 0.049740444868803024, 0.013463023118674755, 0.018203703686594963, -0.01772358827292919, 0.014394019730389118, 0.0466143861413002, 0.03733325004577637, 0.0390690378844738, -0.02082139067351818, -0.004146907944232225, 0.011538231745362282, -0.023435575887560844, -0.055761322379112244, 0.024039238691329956, -0.016355201601982117, -0.032582301646471024, 0.021901335567235947, -0.00767737440764904, 0.04660347104072571, 0.007166353985667229, -0.012145926244556904, -0.02069457806646824, 0.023565543815493584, -0.010150911286473274, 0.030818238854408264, -0.02626287378370762, -0.0001259455893887207, -0.004457508679479361, 0.029692750424146652, 0.014826548285782337, 0.051829759031534195, 0.0781068354845047, -0.013469968922436237, -0.05357823520898819, 0.037199966609478, -0.03893725201487541, 0.05869779735803604, -0.028376305475831032, 0.044769126921892166, -0.028713364154100418, 0.06061236560344696, -0.03215376287698746, 0.05639218911528587, 0.015290929935872555, 0.021244609728455544, 0.03733270987868309, 0.016507284715771675, 0.002241136971861124, -0.04345960170030594, -0.04131270945072174, 0.06610506027936935, 0.01466138195246458, 0.033992744982242584, 0.02396746352314949, 0.02857547625899315, -0.01652853935956955, -0.029910903424024582, -0.07165618240833282, 0.03484977409243584, -0.023177802562713623, 0.026780689135193825, 0.025195321068167686, -0.0010376054560765624, -0.015488128177821636, -0.013372648507356644, -0.003622749587520957, 0.0302280206233263, 0.0114173898473382, 0.027370929718017578, 0.014164509251713753, 0.006168955005705357, 0.004546476993709803, -0.044527191668748856, -0.029199568554759026, 0.008415642194449902, 0.047276824712753296, -0.029277272522449493, -0.02149832807481289, -0.06232261285185814, 0.06031814590096474, -0.03702502325177193, 0.04249010235071182, 0.005861078854650259, -0.034391071647405624, -0.06211503595113754, -0.008618581108748913, 0.06513563543558121, 0.054871007800102234, -3.7120071283425204e-07, 0.01671249233186245, 0.04218088835477829, 0.034685999155044556, -0.015805378556251526, 0.004466226324439049, -0.029203394427895546, -0.0644720122218132, -0.04044784605503082, 0.015195271000266075, 0.01946953870356083, 0.0352729894220829, -0.03296852856874466, -0.04226969927549362, -0.0721067488193512, 0.03707456216216087, -0.028029408305883408, -0.005537306424230337, -0.03958190977573395, 0.02848035655915737, -0.021127592772245407, -0.0037314025685191154, 8.961293497122824e-05, -0.020739974454045296, 0.015995342284440994, -0.004150036256760359, -0.006953506264835596, -0.06420846283435822, -0.03580763563513756, -0.028939228504896164, -0.0006771231419406831, 0.013955656439065933, 0.03589744493365288, -0.026942402124404907, -0.013219564221799374, -0.046198245137929916, 0.011522006243467331, 0.02700803242623806, -0.009049646556377411, 0.027614548802375793, -0.007204877678304911, 0.03769722580909729, -0.03727420046925545, -0.053188201040029526, 0.01923740655183792, -0.051021382212638855, 0.008811935782432556, 0.016130493953824043, 0.049768317490816116, -0.04340846836566925, -0.031713854521512985, 0.02464391104876995, 0.007003164384514093, 0.06956556439399719, -0.03346296027302742, -0.0029818972107023, -0.043942611664533615, 0.01713944971561432, 0.04797331616282463, 0.04731731861829758, 0.027160584926605225, -0.0011475018691271544, 0.04239169880747795, -0.003938606008887291, 0.045642316341400146, -0.04637986421585083, -0.014462527818977833, 0.05491527169942856, 0.05327460542321205, -0.04796009138226509, -0.02975582331418991, 0.021192025393247604, 0.012360358610749245, -0.03853100165724754, -0.0028805003967136145, 0.0798480212688446, 0.007562240120023489, -0.05302755907177925, 0.026138242334127426, -0.01809430681169033, 0.06718356907367706, 0.041001539677381516, 0.04293045774102211, -0.01817665807902813, 0.018154293298721313, -0.013535668142139912, 0.026951396837830544, -0.022636136040091515, -0.039185360074043274, -0.014095217920839787, -0.11844572424888611, 0.0352148711681366, 0.02916075475513935, -0.010641601867973804, -0.04067002236843109, -0.04380795359611511, 0.0218659657984972, -0.02124657668173313, 0.014122146181762218, 0.02727852575480938, -0.04338075965642929, -0.03476543724536896, 0.04011224955320358, -0.020801838487386703, -0.07441377639770508, -0.0032464496325701475, -0.017077624797821045, 0.06843913346529007, 0.016420414671301842, 0.04115365073084831, -0.04648832976818085, -0.010372944176197052, 0.01591307297348976, -0.05032799392938614, -0.002694580005481839, -0.0399320051074028, 0.016061674803495407, -0.004231291823089123, -0.008731517940759659, 0.031051598489284515, 0.002660989761352539, 0.009187953546643257, -0.004563136957585812, -0.014406729489564896, 0.07282291352748871, -0.004894420970231295, -0.020342377945780754, 0.017534129321575165, -0.019860394299030304, -0.04423338919878006, 0.041622865945100784, 0.0034973081201314926, -0.034560058265924454, -0.0013155617052689195, -0.00437388988211751, 0.0016148271970450878, -0.025101391598582268, -0.027989888563752174, 0.0034189235884696245, -0.04735172912478447, 0.0062942225486040115, -0.045744169503450394, 0.026119541376829147, 0.03176378458738327, 0.043823227286338806, 0.0099535146728158, 0.04882413148880005, -0.013513787649571896, 0.014279537834227085, -0.012507244944572449, 0.013704934157431126, 0.000979009666480124, -0.02999144047498703, -0.06081201136112213, -0.024158991873264313, 0.04868711531162262, -0.005372925661504269, -0.036531735211610794, -0.027962518855929375, -0.041109129786491394, -0.06119871512055397, 1.2370755939627998e-06, 0.04047028720378876, 0.029264798387885094, 0.017944563180208206, 0.0038734180852770805, -0.08968700468540192, 0.010052946396172047, 0.0007002436323091388, 0.027474775910377502, 0.05516483262181282, 0.009003235027194023, -0.026401273906230927, -0.03947253152728081, 0.028538357466459274, -0.03390004113316536, 0.03835649788379669, 0.02949996292591095, 0.028074029833078384, 0.022505469620227814, -0.04866253584623337, 0.021063297986984253, 0.029785435646772385, 0.04188380017876625, 0.017590120434761047, -0.06803227961063385, -0.03195361793041229, 0.05818195641040802, -0.04887089133262634, 0.007501581218093634, -0.017005670815706253, -0.007636497728526592, 0.02887667529284954, 0.014932414516806602, -0.06311234086751938, -0.007067886646836996, 0.03319288790225983, 0.033030129969120026, -0.09536873549222946, 0.012100842781364918, -0.002270015887916088, -0.018667034804821014, 0.03812866657972336, 0.006247700657695532, -0.01699065789580345, 0.02259460650384426, 0.04694214463233948, 0.025275250896811485, -0.008290598168969154, 0.008416494354605675, 0.023821929469704628, -0.012055146507918835, -0.04895582050085068, 0.056038953363895416, -0.05675128847360611, -0.0676679015159607, -0.0721173956990242, 0.010234178975224495, 0.06042267382144928, -0.007706493604928255, 0.061865657567977905, 0.0017493076156824827, -0.04289592057466507, -0.0028993794694542885, -0.017637033015489578, 0.03892679885029793, 0.02591322734951973, -0.0015218732878565788, 0.008845575153827667, -0.012624267488718033, 0.04959610477089882, 0.04748740792274475, 0.08922304213047028, 0.055453263223171234, -0.0039447336457669735, -0.017888369038701057, -0.03321276605129242, -0.0064534773118793964, -0.029104970395565033, 0.036016881465911865, -0.0019632726907730103, 0.04016362875699997, -0.018985169008374214, -0.0513363741338253, -0.03336317464709282, -0.009352359920740128, 0.026719501242041588, -0.03827137500047684, -0.017077935859560966, 0.04633365944027901, -0.024623597040772438, -0.03749609738588333, -0.026998288929462433, -0.03793688863515854, 0.019051626324653625, 0.000856245867908001, 0.010038246400654316, -6.220874796288907e-33, -0.05064081400632858, 0.010164223611354828, -0.01666143909096718, -0.00720710214227438, -0.022134479135274887, -0.021797697991132736, 0.030695484951138496, 0.006269370671361685, -0.004395767115056515, -0.021922459825873375, -0.04479805380105972, -0.0012865549651905894, 0.012891886755824089, -0.0014633249957114458, -0.004747514147311449, 0.038160305470228195, 0.04338124766945839, -0.01628037914633751, -0.0140679981559515, 0.014104776084423065, 0.015828590840101242, 0.04609326273202896, 0.07618358731269836, -0.018841180950403214, -0.023961249738931656, 0.0330982580780983, -0.03394807502627373, -0.00939808040857315, -0.02083015628159046, 0.024591250345110893, -0.024554988369345665, 0.03394860029220581, 0.026718124747276306, -0.025868510827422142, -0.0182507187128067, 0.002166101709008217, -0.049506377428770065, 0.011974076740443707, 0.0352105051279068, -0.0018587535014376044, 0.012985477223992348, -0.010113850235939026, 0.022619469091296196, 0.029318377375602722, -0.06605847924947739, -0.004256490617990494, 0.016387643292546272, -0.06005985662341118, -0.020024562254548073, -0.022308021783828735, -0.06549148261547089, 0.03598203882575035, 0.013147355057299137, 0.035691846162080765, 0.026873696595430374, 0.020667217671871185, -0.011847029440104961, 0.01951763965189457, -0.019769564270973206, -0.0027911649085581303, 0.047705765813589096, 0.11223981529474258, 0.018267540261149406, 0.020355328917503357, -0.05124277248978615, 0.02017597109079361, -0.01540500670671463, 0.012123764492571354, 0.059118933975696564, -0.02053399570286274, 0.03313744440674782, 0.04485105723142624, 0.04132461920380592, 0.008200054988265038, 0.01721770130097866, -0.03728104010224342, -0.042299214750528336, -0.011778884567320347, -0.01332329772412777, -0.013436810113489628, 0.039187751710414886, 0.0340123288333416, -0.0285195242613554, 0.0035663116723299026, -0.03873768448829651, -0.021024320274591446, -0.03941129520535469, -0.006490690633654594, 0.0374697707593441, 0.0055628991685807705, -0.0101690161973238, 0.034798357635736465, 0.03131731227040291, -0.049367304891347885, -0.022736048325896263, -0.008230510167777538, -0.009086071513593197, 0.009918143041431904, -0.009642418473958969, 0.03073195368051529, -0.03234118968248367, -0.056962255388498306, 0.034987855702638626, -0.003547586966305971, 0.009554262273013592, -0.05346338450908661, 0.004637662321329117, 0.022942259907722473, -0.0912879928946495, -0.0014500912511721253, 0.01491958275437355, 0.0031081351917237043, -0.0035509848967194557, 0.0849066823720932, -0.006297259125858545, -0.016916297376155853, 0.026592517271637917, 0.06841585785150528, 0.04661617800593376, -0.015793051570653915, -0.010601960122585297, -0.02409985288977623, 0.034771960228681564, 0.01900215819478035, -0.01626368798315525, 0.01300832349807024, 0.0075520602986216545, 0.02487483061850071, 0.03219061717391014, -0.04969315603375435, -0.016982626169919968, -0.002959214150905609, 2.6165477606809873e-07, 0.03691098093986511, 0.1037776917219162, -0.041473932564258575, -0.06271568685770035, 0.027503959834575653, -0.00159990054089576, -0.019539374858140945, 0.05453704670071602, 0.02933654561638832, 0.03604304790496826, 0.021719148382544518, -0.02899819053709507, 0.029687270522117615, -0.024622032418847084, -0.025390924885869026, -0.023466968908905983, -0.08952523022890091, 0.027774933725595474, -0.09008815139532089, -0.02657320722937584, 0.006993862334638834, 0.12749066948890686, -0.0066941361874341965, 0.0018651477294042706, -0.00340713607147336, -0.045132726430892944, 0.03287355974316597, -0.05968493968248367, -0.00751850288361311, 0.030218441039323807, 0.1022181287407875, 0.02027951553463936, -0.005481812637299299, 0.024566449224948883, -0.02903597615659237, 0.0962415263056755, -0.0003306573780719191, 0.05270342156291008, -0.04753433167934418, 0.021242111921310425, 0.009929629974067211, 0.045988909900188446, 0.006121183279901743, -0.08753407746553421, 0.07939118146896362, -0.03300631046295166, -0.029758429154753685, -0.004802567884325981, -0.043604299426078796, -0.02692069113254547, 0.004077422432601452, -0.03749996796250343, -0.016721295192837715, 0.016284670680761337, 0.03316641226410866, 0.00018718469073064625, -0.0428721085190773, -0.07779798656702042, 0.027932802215218544, 0.03962752968072891, -0.0425528921186924, -0.08958116918802261, -0.01335875689983368, -0.04027378186583519, 0.06208100542426109, -0.06969401985406876, -0.0026349469553679228, 2.648313237707024e-34, 0.029567616060376167, -0.0031691088806837797, 0.02634032629430294, 0.008033732883632183, 0.02500792220234871, -0.014391029253602028, 0.030980968847870827, 0.024585716426372528, 0.009861277416348457, -0.1302994191646576, -0.024333912879228592], [-0.0023801515344530344, 0.07729341834783554, -0.002120638033375144, 0.04792653024196625, 0.08731358498334885, 0.049682628363370895, 0.010999034158885479, 0.006293002050369978, -0.023304268717765808, -0.02163216844201088, -0.051182471215724945, 0.03658194839954376, -0.026304779574275017, 0.0299695897847414, 0.024393215775489807, -0.06323077529668808, 0.008414158597588539, -0.008660630322992802, -0.019096355885267258, -0.021976381540298462, -0.023751193657517433, 0.024989163503050804, 0.005201627966016531, 0.012216296046972275, 0.04738031327724457, 7.907374674687162e-05, -0.06165892258286476, 0.019408579915761948, 0.041362181305885315, 0.02368967980146408, 0.007135275285691023, 0.051821593195199966, 0.0148525545373559, 0.03540530055761337, 1.9077392607869115e-06, -0.006153955589979887, -0.04106462374329567, -0.012455631978809834, -0.036890365183353424, 0.07286673039197922, 0.04831895977258682, 0.006264711730182171, 0.0015494376420974731, 0.019874809309840202, -0.018809298053383827, 0.008910019882023335, 0.023906495422124863, 0.038212936371564865, 0.05002638325095177, 0.04749802127480507, -0.01705239899456501, -0.017372727394104004, 0.014886249788105488, -0.030160246416926384, -0.06023726984858513, -0.054190345108509064, 0.012483707629144192, -0.03530098870396614, -0.07193773239850998, -0.017102111130952835, -0.018260719254612923, 0.018258778378367424, 0.005256984382867813, 0.04260033741593361, 0.014727022498846054, -0.00876390840858221, -0.04573001712560654, 0.00196024589240551, 0.003457366954535246, 0.06393765658140182, -0.10997425019741058, -0.021642394363880157, 0.014564283192157745, -0.03327265381813049, 0.03463206812739372, -0.06776328384876251, -0.00831869151443243, -0.025024546310305595, 0.0276208333671093, 0.016471432521939278, -0.033634547144174576, -0.04625329747796059, 0.016849976032972336, 0.008264051750302315, -0.0317712239921093, 0.0023051907774060965, 0.014878286980092525, 0.013595419004559517, -0.0363520011305809, -0.05566324666142464, -0.04041869193315506, -0.004360540304332972, -0.03412288799881935, -0.009968386963009834, -0.044937219470739365, -0.0485822819173336, -0.058974672108888626, -0.03047737292945385, 0.0015712741296738386, -0.003926252946257591, 0.10851611942052841, 0.007978379726409912, 0.0019222060218453407, 0.025142738595604897, -0.056779757142066956, 2.0719267922686413e-05, -0.03577665239572525, 0.016507143154740334, -0.01761283352971077, -0.0026536572258919477, -0.015157640911638737, 0.04511849954724312, -0.029393842443823814, 0.009956437163054943, -0.05444274842739105, -0.005546266213059425, -0.019188549369573593, 0.02660210430622101, 0.003832805436104536, -0.005574725102633238, -0.015643589198589325, 0.04113586246967316, -0.009343008510768414, -0.02269865944981575, 0.008888006210327148, 0.03175383061170578, -0.007246464490890503, 0.045024286955595016, -0.026282384991645813, 0.0021491916850209236, -0.005457248073071241, -0.016132552176713943, 0.06641998887062073, 0.008452903479337692, 0.0879235565662384, 0.014165258966386318, -0.015759868547320366, -0.022575028240680695, -0.02369360439479351, 0.018503744155168533, 0.016609350219368935, -0.03630321845412254, -0.008641748689115047, -0.0009596757590770721, 0.012534276582300663, -0.00039403818664141, 0.05109043046832085, -0.029813241213560104, -0.013978544622659683, -0.04472317174077034, -0.07742615789175034, 0.07007850706577301, 0.009326825849711895, -0.015530942007899284, -0.0003463804314378649, -0.03704974055290222, -0.010587687604129314, 0.035623401403427124, 0.03581463545560837, 0.052639156579971313, -0.02592114731669426, -0.05040268227458, -0.03316454216837883, 0.005204228684306145, -0.00902929250150919, 0.026234664022922516, 0.011438199318945408, 0.004029660020023584, 0.02468443475663662, 0.005482903216034174, 0.03895305097103119, -0.03660133108496666, 0.00246816617436707, -0.03341272845864296, 0.04612438380718231, 0.006611619144678116, 0.07708205282688141, 0.09906694293022156, 0.0011707982048392296, -0.03176632151007652, 0.005053010769188404, -0.046461351215839386, -0.02893598936498165, 0.0604538731276989, -0.053921133279800415, -0.01714923046529293, -0.006069962400943041, 0.055733077228069305, -0.0555916465818882, -0.03402823582291603, 0.020008737221360207, 0.024809319525957108, -0.042877715080976486, 0.03466518968343735, 0.01775394193828106, -0.02521628700196743, -0.030321095138788223, 0.01062170509248972, 0.054836153984069824, -0.03187917545437813, -0.04603921249508858, -0.03923638537526131, 0.017170028761029243, 0.02737608179450035, 0.0016988345887511969, -0.034326933324337006, -0.023378653451800346, -0.028742803260684013, 0.07002051919698715, -0.03134673833847046, 0.03966803476214409, 0.008519354276359081, 0.05620983988046646, -0.030076896771788597, -0.015975914895534515, 0.004800734110176563, 0.009286939166486263, 0.04494375362992287, -0.044187020510435104, 0.013027122244238853, 0.019478458911180496, 0.004421416670084, -0.05258418619632721, 0.0006736204959452152, -0.01980092190206051, -0.031300537288188934, 0.04728291183710098, 0.020429562777280807, 0.037554189562797546, -0.02218998782336712, -0.0011489663738757372, -0.05710006505250931, 0.03321307152509689, -0.04774756729602814, 0.039674319326877594, -0.039340175688266754, 0.020732533186674118, 0.023193059489130974, 0.012724888511002064, 0.015661533921957016, 0.0448245145380497, 0.08508835732936859, -0.046695858240127563, -0.05819370970129967, 0.023599321022629738, -0.01556339394301176, -0.01733984239399433, -0.010249396786093712, 0.01907573826611042, -0.020125698298215866, 0.05518396943807602, -0.010147134773433208, 0.0026444229297339916, 0.0230262391269207, -0.016560081392526627, 0.011716225184500217, 0.02526034042239189, 0.020428936928510666, -0.05735602602362633, -0.03527679294347763, -0.02150876261293888, 0.0030275958124548197, 0.03683025762438774, -0.021013721823692322, 0.006827371660619974, 0.02485317923128605, -0.01974693313241005, -0.04898536950349808, 0.034560322761535645, -0.018442220985889435, -0.0035256114788353443, 0.023840969428420067, 0.006592477206140757, 0.019409561529755592, -0.042463526129722595, 0.002391239395365119, 0.08728247880935669, -0.04577705264091492, -0.030507784336805344, 0.002638581907376647, 0.01783641427755356, -0.030779924243688583, -0.041169844567775726, -0.003968818113207817, 0.018953176215291023, 0.019019363448023796, -0.018824292346835136, 0.0012651332654058933, -0.04766124114394188, 0.020567649975419044, -0.008907412178814411, 0.04770081117749214, -0.015830431133508682, -0.06604897230863571, -0.04826854541897774, -0.029023723676800728, -0.03249078616499901, 0.10779636353254318, 0.010151430033147335, -0.029919106513261795, 0.05893265828490257, 0.03014906495809555, -0.028004257008433342, 0.03407999873161316, -0.0066778939217329025, -0.05072125047445297, -0.0036971550434827805, 0.025465646758675575, -0.004775495734065771, 0.001978459535166621, -0.032759979367256165, 0.0038509895093739033, -0.05210589990019798, 0.05006898567080498, 0.03064028173685074, -0.009984227828681469, -0.06971291452646255, 0.06664762645959854, -0.012392805889248848, -0.012994765304028988, -0.010836849920451641, 0.0037541151978075504, -0.00233412510715425, -0.020606646314263344, 0.005845701787620783, 0.01246780063956976, -0.045468010008335114, -0.009287222288548946, -0.029120011255145073, -0.004255407024174929, 0.051800020039081573, -0.0062890490517020226, 0.009210445918142796, -0.037015918642282486, 0.008415286429226398, 0.0535203255712986, -0.008117446675896645, 0.014646224677562714, 0.015818076208233833, 0.0035361943300813437, -0.03980467841029167, -0.08230095356702805, 0.007123111747205257, -0.029779911041259766, 0.008606276474893093, 0.01690010167658329, 0.01659538969397545, -0.047514911741018295, -0.016214704141020775, 0.011968809179961681, 0.013853153213858604, 0.07003586739301682, -0.019033456221222878, -0.006994225550442934, -0.058281008154153824, 0.0066053494811058044, 0.07288381457328796, 0.060681868344545364, 0.03503625467419624, -0.0017335166921839118, 0.0664377212524414, -0.02447652816772461, 0.024453861638903618, -0.00958744902163744, -0.06856471300125122, 0.050443135201931, -0.019167276099324226, 0.015858765691518784, -0.00026812966098077595, 0.003884147386997938, -0.025333330035209656, -0.027978714555501938, 0.015760503709316254, 0.06722088158130646, -0.007409785874187946, -0.04652327671647072, -0.009626571089029312, 0.006311004050076008, 0.03577789291739464, 0.023479804396629333, 0.035487040877342224, 0.006786330137401819, -0.03074846975505352, 0.013166619464755058, 0.05278996378183365, 0.014852338470518589, -0.0017423582030460238, 0.026076514273881912, -0.02627430111169815, 0.054532241076231, 0.026198923587799072, -0.03301313892006874, -0.06863272190093994, -0.004657778423279524, 0.002629329217597842, -0.03681199997663498, 0.007387753576040268, 0.03584490716457367, -0.04759729281067848, -0.04346103221178055, 0.024742890149354935, 0.0026751209516078234, -0.015808383002877235, 0.016281282529234886, -0.027547111734747887, 0.012888291850686073, -0.023803316056728363, 0.03488015756011009, -0.006996419746428728, -0.026586361229419708, -0.0009088221122510731, -0.027537347748875618, -0.0012266342528164387, -0.037665802985429764, 0.03703271225094795, 0.02134143002331257, -0.05166073143482208, 0.0358835905790329, 0.050173744559288025, -0.003581002587452531, 0.010765144601464272, 0.000304594257613644, 0.03825540840625763, -0.02956179343163967, 0.023390622809529305, 0.02924448810517788, -0.05944281443953514, -0.03904919698834419, 1.6657399100949988e-05, 0.026321299374103546, -0.03014327399432659, -0.0010854077991098166, 0.007867361418902874, -0.05588865652680397, 0.00030504423193633556, -0.0007684387965127826, -0.011391286738216877, -0.04660125449299812, -0.015354280360043049, -0.025172261521220207, 0.043876562267541885, 0.03008338250219822, 0.0589049868285656, -0.023216983303427696, -0.02175338752567768, -0.006188685540109873, 0.02178402990102768, -0.058334749191999435, 0.03979910910129547, -0.023684551939368248, -0.032463572919368744, -0.0279556754976511, -0.018103046342730522, 0.059159159660339355, -0.01222515944391489, -0.011180982924997807, 0.009149878285825253, -0.07011584937572479, -0.03601166605949402, -0.009347488172352314, 0.054567232728004456, 0.008895264938473701, 0.03388672322034836, -0.01447113137692213, -0.08014319837093353, 0.026636365801095963, 0.0073908232152462006, 0.02755097672343254, 0.06558545678853989, 0.03586795553565025, -0.02685127966105938, -0.022081123664975166, 0.03183501586318016, -0.07666569203138351, 0.01711723953485489, 0.02993783727288246, -0.03160843998193741, -0.030037958174943924, -0.03651510179042816, -0.018117515370249748, 0.033605631440877914, 0.01743248663842678, 0.04222790151834488, -0.062159791588783264, -0.029690273106098175, 0.055433835834264755, -0.002613880205899477, 0.018296556547284126, -0.03120383806526661, -0.008832109160721302, 0.026373708620667458, 0.03144322335720062, -0.04378923401236534, 0.014268090948462486, 0.07207229733467102, 0.0003066686913371086, -0.05956146866083145, -0.014676409773528576, -0.011174655519425869, 0.02194867841899395, 0.032162636518478394, -0.015423213131725788, -0.007296554744243622, 0.04833464324474335, 0.07325130701065063, 0.003540466073900461, -0.014428302645683289, -0.008242127485573292, -0.004639435559511185, 0.02744448557496071, -0.1347227245569229, 0.054705288261175156, -0.03679266571998596, -0.12850669026374817, -0.06910408288240433, 0.011483602225780487, 0.05874455347657204, 0.02205663174390793, 0.05752963200211525, 0.02998150885105133, -0.044624004513025284, -0.016874998807907104, 0.012694746255874634, 0.01668776385486126, 0.05476749315857887, 0.0244013462215662, 0.007722042966634035, 0.0031229995656758547, 0.02394079603254795, 0.016914688050746918, 0.04524320363998413, 0.05974121391773224, -0.019517185166478157, 0.011548821814358234, -0.03080884926021099, 0.02109173871576786, 0.013716877438127995, 0.012089215219020844, -0.007939005270600319, -0.0012361678527668118, 0.049949757754802704, -0.016360579058527946, 0.004326458554714918, -0.03186345845460892, 0.008547934703528881, -0.051105983555316925, 0.02196330949664116, -0.0013709153281524777, -0.01504350546747446, -0.015133589506149292, -0.00013886690430808812, -0.0535757839679718, 0.0055687506683170795, -0.0060646021738648415, 0.006848692893981934, -6.20711710428047e-33, -0.015109878033399582, 0.04721752554178238, -0.020308230072259903, 0.030297212302684784, 0.02251281589269638, -0.018857788294553757, 0.003514415118843317, 0.011003361083567142, -0.023211393505334854, -0.005860034842044115, -0.013554643839597702, 0.015351464971899986, 0.008347516879439354, -0.019818168133497238, 0.02693679928779602, -0.006903135683387518, 0.09346803277730942, -0.010269662365317345, -0.00726828770712018, 0.0243587177246809, 0.025037605315446854, 0.029976239427924156, -0.009187876246869564, -0.04967821389436722, -0.10347102582454681, 0.033846378326416016, -0.05508929863572121, -0.017758795991539955, -0.018337901681661606, 0.009609873406589031, -0.02032831311225891, 0.02999376691877842, -0.005827976390719414, 0.023398078978061676, -0.01077281218022108, -0.004274019971489906, -0.06873740255832672, 0.01239313930273056, -0.01471364963799715, -0.00952183548361063, 0.037128206342458725, -0.04651825875043869, 0.06016154959797859, 0.045260969549417496, -0.033666010946035385, -0.028154028579592705, 0.027907071635127068, -0.035511139780282974, -0.009061504155397415, -0.050516024231910706, -0.03037162311375141, -0.0017079708632081747, -0.01228667888790369, -0.01750612072646618, 0.042692504823207855, 0.0345163494348526, -0.03490232676267624, -0.0009650036809034646, -0.03300301730632782, -0.04412565007805824, 0.0597066730260849, 0.05303585156798363, -0.031141091138124466, 0.030057063326239586, -0.016690285876393318, -0.003885073820129037, -0.024954047054052353, 0.014584633521735668, 0.03943420201539993, 0.01837613806128502, 0.04431706294417381, 0.08052383363246918, 0.05126195773482323, 0.04064055159687996, 0.007883600890636444, -0.022520435974001884, -0.0732094943523407, 0.00833879318088293, -0.008167491294443607, 0.004292907193303108, -0.003902455559000373, 0.008583301678299904, 0.02531096525490284, 0.019052062183618546, -0.01878700591623783, -0.03482441604137421, -0.044937219470739365, 0.016019385308027267, 0.03528407961130142, 0.045076947659254074, -0.030403710901737213, -0.017266597598791122, 0.021531423553824425, -0.05034724995493889, 0.06350476294755936, -0.006010456942021847, -0.007139743305742741, 0.0039979517459869385, 0.03605116903781891, -0.013141523115336895, -0.012992890551686287, -0.11196697503328323, 0.05037225782871246, -0.01628609001636505, 0.0229310542345047, -0.025807470083236694, 0.028144212439656258, 0.04801303893327713, -0.035697318613529205, -0.0033558846917003393, 0.009152290411293507, 0.00519156688824296, -0.0003386134631000459, 0.07673387974500656, -0.010302194394171238, -0.03428693115711212, 0.03792588785290718, 0.029474150389432907, -0.03222649171948433, 0.037713032215833664, -0.023030370473861694, -0.02710641548037529, 0.05566155165433884, 0.032697971910238266, -0.026182858273386955, 0.0033830583561211824, -0.034076038748025894, -0.03180619329214096, 0.033787067979574203, -0.056301675736904144, -0.007560207042843103, 0.003958664834499359, 2.613401761664136e-07, -0.01786002889275551, 0.06627152860164642, -0.04854440689086914, -0.07634492963552475, 0.00974420178681612, 0.03648378700017929, -0.004119556862860918, 0.03166215866804123, 0.03457274287939072, 0.019703829661011696, 0.05021040886640549, -0.016457514837384224, 0.04493984952569008, -0.013843836262822151, -0.04438945651054382, -0.0314619280397892, -0.02718873880803585, 0.051237087696790695, -0.06305057555437088, -0.03937656804919243, -0.03164875507354736, 0.11763923615217209, 0.019354136660695076, -0.014293436892330647, 0.05970658361911774, 0.00015247307601384819, 0.0268119927495718, -0.01384039781987667, 0.004382781218737364, -0.01476176455616951, 0.06317952275276184, 0.027164015918970108, -0.047838032245635986, -0.02230929769575596, -0.005330415442585945, 0.09027504920959473, -0.03721030801534653, 0.057293850928545, -0.0330859050154686, -0.03163070231676102, 0.05335884541273117, 0.015124576166272163, 0.03413527458906174, -0.06356728821992874, 0.04966619983315468, -0.013472091406583786, 0.0033571794629096985, -0.04327206313610077, -0.001584565849043429, 0.003358714049682021, 0.005242551676928997, -0.06098198518157005, 0.03829104080796242, 0.008275201544165611, 0.0072287521325051785, -0.045701999217271805, -0.010648299008607864, -0.09520411491394043, 0.03738517686724663, 0.04085026681423187, -0.02103082835674286, -0.04472077637910843, -0.03684617206454277, -0.04385882988572121, 0.08224601298570633, -0.043400444090366364, 0.018866827711462975, 2.1277767885140675e-34, 0.01761998049914837, -0.01487493421882391, 0.06015629693865776, -0.00039464497240260243, -0.009122402407228947, 0.0206485353410244, -0.01065038237720728, 0.013881901279091835, 0.0457499735057354, -0.09655768424272537, -0.018471697345376015], [-0.009036659263074398, 0.08558572828769684, -0.016071829944849014, 0.061109598726034164, -0.03468814119696617, 0.02309083566069603, 0.022060224786400795, 0.006640622857958078, -0.001573850866407156, 0.019873525947332382, 0.009046325460076332, 0.016541803255677223, 0.022350983694195747, 0.015203926712274551, 0.023204704746603966, -0.0738258808851242, 0.01497570239007473, -0.03672047704458237, -0.016708966344594955, -0.030361441895365715, -0.0011919356184080243, 0.004738271702080965, 0.02225359156727791, 0.012075484730303288, 0.009281830862164497, -0.053387127816677094, -0.04439222067594528, 0.03757057338953018, 0.06279663741588593, -0.007046794053167105, -0.0016531216679140925, 0.03383276239037514, 0.03082788735628128, 0.03968933969736099, 2.1816561002196977e-06, -0.012831460684537888, -0.02580135129392147, 0.04560544714331627, 0.0099959010258317, 0.05208753049373627, 0.020626304671168327, 0.0003337641537655145, 0.02714383415877819, 0.013097834773361683, -0.01276981271803379, -0.04195478558540344, 0.046906571835279465, -0.01851520501077175, -0.023809712380170822, 0.0319904200732708, -0.009580491110682487, -0.00894973985850811, 0.05047941207885742, -0.06303612887859344, 0.008390822447836399, -0.06214962527155876, -0.0009303837432526052, -0.024683134630322456, -0.04755966365337372, -0.06461001187562943, -0.014604168012738228, 0.02835877239704132, 0.05248279869556427, 0.030450409278273582, -0.035056982189416885, 0.011431440711021423, -0.015546375885605812, -0.05319450795650482, -0.015487927943468094, -0.0006156940362416208, -0.07460074871778488, -0.02158893644809723, 0.014840108342468739, -0.01856035180389881, -0.038901329040527344, 0.006335145328193903, -0.02640892192721367, 0.017980966717004776, -0.036757323890924454, -0.005092298146337271, 0.004761470016092062, -0.005056343507021666, 0.03677754104137421, -0.01445711962878704, -0.026957280933856964, 0.02207835763692856, 0.06571004539728165, 0.0397120863199234, 0.005440474487841129, 0.018359187990427017, -0.0596868172287941, -0.052936963737010956, -0.01580505445599556, -0.02561386488378048, -0.010013782419264317, -0.031180525198578835, -0.07413985580205917, 0.04400104656815529, 0.028256289660930634, 0.007695232518017292, 0.09150394797325134, 0.05731203407049179, -0.005015566945075989, 0.009227803908288479, -0.030618760734796524, -0.004260667599737644, -0.07282175868749619, -0.014155803248286247, 0.009220445528626442, -0.007127821445465088, 0.011340279132127762, -0.015819672495126724, -0.008581998758018017, 0.029528576880693436, -0.029554160311818123, 0.002452889923006296, -0.03829912841320038, -0.003882717341184616, -0.01799883507192135, -0.029689686372876167, -0.018051523715257645, 0.033568043261766434, -0.0019709584303200245, 0.05402585491538048, -0.03389979526400566, 0.047585561871528625, -0.04268629476428032, 0.03134145587682724, -0.011598089709877968, -0.07627808302640915, -0.012684086337685585, -0.0008946811431087554, -0.037926554679870605, -0.022946380078792572, 0.05392724648118019, 0.056680791079998016, 0.03939400985836983, -0.05909828841686249, -0.046448107808828354, 0.018648818135261536, 0.03592770919203758, -0.12767839431762695, 0.029970912262797356, -0.04486182704567909, 0.0402572900056839, -0.007258315570652485, 0.01776915416121483, 0.007782049477100372, -0.001446023117750883, 0.017210861667990685, -0.028236908838152885, 0.07880176603794098, -0.04589761421084404, -0.047034841030836105, 0.041020579636096954, -7.729930803179741e-05, 0.019859228283166885, 0.06744121760129929, 0.042640320956707, 0.06074615567922592, -0.005248165223747492, 0.0024450202472507954, -0.009386305697262287, 0.02926964871585369, 0.007441704161465168, 0.019903311505913734, -0.039699774235486984, 0.004599651787430048, 0.0016642882255837321, 0.00446283956989646, 0.05966668203473091, 0.007921943441033363, -0.028586814180016518, -0.022329438477754593, 0.027988160029053688, 0.033545415848493576, 0.0318404845893383, 0.02731400914490223, 0.025205098092556, -0.035132527351379395, 0.016908029094338417, -0.022138692438602448, -0.0454430878162384, 0.09009309113025665, -0.031032700091600418, 0.019572030752897263, 0.028277991339564323, 0.05034150928258896, -0.023072896525263786, -0.03757739067077637, -0.00807168148458004, -0.0050035761669278145, -0.04581610485911369, 0.01935696229338646, 0.033624209463596344, -0.021618522703647614, -0.008366475813090801, 0.023892678320407867, 0.06332462280988693, -0.018530622124671936, -0.07013630867004395, -0.0997999832034111, 0.06489808112382889, 0.03126836195588112, -0.005369093269109726, -0.04017014801502228, -0.07421588152647018, 0.02260785549879074, 0.012350432574748993, 0.02814517356455326, 0.017596788704395294, 0.039068229496479034, 0.03826778009533882, -0.021862884983420372, -0.006736093200743198, -0.012864340096712112, 0.023077845573425293, 0.037950366735458374, 0.06654983013868332, 0.026581035926938057, -0.0028067328967154026, -0.039750613272190094, 0.0059845405630767345, 0.020445697009563446, 0.013867291621863842, 0.01123901642858982, 0.02358807437121868, -0.0052377087995409966, 0.01814635843038559, -0.025787143036723137, -0.020612282678484917, -0.04277726262807846, 0.045827049762010574, -0.016545714810490608, 0.05279108136892319, -0.028423363342881203, -0.0010795287089422345, -0.020717868581414223, 0.02089986763894558, -0.008907497860491276, 0.009364228695631027, 0.05703602358698845, -0.0645618662238121, -0.036564234644174576, 0.028984487056732178, -0.0077875638380646706, 0.024881940335035324, -0.027241278439760208, -0.012224504724144936, 0.003274130169302225, 0.04680750519037247, 0.008552785031497478, 0.021063460037112236, -0.01538001373410225, -0.0013143457472324371, 0.05591093376278877, 0.03699475899338722, 0.02943645790219307, -0.040678977966308594, -0.006439258344471455, 0.018154507502913475, 0.00405698549002409, -0.019419515505433083, 0.007313231937587261, -0.0005507490714080632, 0.018428398296236992, -0.0013477539177984, -0.03231712430715561, 0.059362106025218964, -0.019207146018743515, 0.04931890591979027, -0.0016666804440319538, 0.01096261478960514, -0.01515735313296318, -0.012968887574970722, 0.03289177641272545, 0.04201491177082062, -0.04357415810227394, 0.03226334974169731, -0.020062940195202827, -0.011582856066524982, -0.04808686301112175, -0.04718147590756416, -0.04184555262327194, 0.002974387491121888, 0.0680513083934784, 0.021579552441835403, -0.060669947415590286, 0.0035663084127008915, 0.015843885019421577, -0.03201902657747269, 0.049392107874155045, -0.03151845559477806, -0.014972936362028122, 0.026891566812992096, -0.01214202493429184, 0.04744899272918701, 0.0791681632399559, -0.014544025994837284, 0.010791844688355923, 0.025813205167651176, 0.0074835955165326595, -0.018260743468999863, 0.022804388776421547, -0.004226003773510456, -0.05945022776722908, 0.005598100833594799, -0.010323370806872845, -0.05005735531449318, 0.06677677482366562, -0.009336867369711399, -0.010472540743649006, -0.019035950303077698, -0.0060896058566868305, -0.00840016733855009, -0.043333351612091064, 0.019371749833226204, 0.04405608028173447, 0.006019336171448231, -0.019798211753368378, -0.022055109962821007, -0.06727379560470581, 0.02901901677250862, -0.04104723408818245, 0.011440540663897991, 0.04061191901564598, -0.008436189033091068, -0.03413566201925278, 0.04801233112812042, 0.008687054738402367, 0.013857281766831875, -0.05051901936531067, -0.0021080374717712402, 0.029265200719237328, -0.019650794565677643, 0.005928588565438986, -0.0042982022278010845, 0.0065322499722242355, -0.08767899125814438, 0.03541649132966995, -0.026136910542845726, -0.0416097491979599, 0.032533615827560425, 0.03455851227045059, -0.010090515948832035, 0.024272838607430458, -0.035143908113241196, -0.05550762265920639, -0.04588935524225235, 0.023148195818066597, -0.002731075044721365, 0.06359457969665527, -0.060265280306339264, -0.008568025194108486, -0.016080034896731377, -0.0002760301576927304, 0.053592562675476074, 0.00813536811619997, -0.013522629626095295, -0.04253115504980087, -0.00012428029731381685, 0.03928981348872185, 0.02034180425107479, 0.03157167509198189, -0.10911619663238525, 0.05725717544555664, 0.0036407236475497484, -0.0005412957980297506, -0.03641708567738533, 0.03295981511473656, 0.019298039376735687, -0.06728550046682358, 0.002056174213066697, 0.05665663629770279, -0.007202803622931242, -0.0988999679684639, -0.033188991248607635, -0.023338710889220238, 0.07504744827747345, 0.03403816744685173, 0.01768733561038971, -0.05752360448241234, -0.05377973988652229, 0.017572471871972084, 0.02003283053636551, 6.499345181509852e-05, -0.009482720866799355, 0.03220381587743759, -0.038585782051086426, 0.05099215358495712, -0.025824973359704018, -0.06565897166728973, -0.046758271753787994, -0.07218590378761292, -0.015428083948791027, -0.03620864450931549, -0.019556770101189613, -0.029020145535469055, -0.01603148691356182, 0.022014552727341652, 0.0023578638210892677, -0.017358699813485146, 0.03717270493507385, -0.003727902891114354, -0.03343810886144638, -0.011282735504209995, 0.06455449014902115, 0.008498983457684517, 0.0012737768702208996, -0.04175763204693794, 0.021476346999406815, 0.011373606510460377, 0.021676575765013695, -0.041446179151535034, 0.0908612608909607, 0.013485700823366642, -0.022576749324798584, -0.0560363344848156, 0.03810492902994156, 0.013973726890981197, -0.030856916680932045, -0.0036099369172006845, 0.03026854433119297, 0.005614911206066608, -0.017661402001976967, -0.00427895225584507, -0.024681370705366135, -0.009981508366763592, 0.025154883041977882, 0.03688549995422363, 0.006952687166631222, -0.08830495923757553, -0.029422493651509285, -0.014323082752525806, -0.007988997735083103, 0.0031975472811609507, -0.01754414476454258, 0.02942460961639881, -0.07596660405397415, 0.029987424612045288, 0.004416435491293669, 0.03576090931892395, 0.04271284118294716, -0.03514247015118599, -0.006007499992847443, -0.022326482459902763, -0.0038351307157427073, -0.09477353096008301, 0.05282868817448616, -0.022651799023151398, 0.011649559251964092, -0.032524529844522476, -0.02221013233065605, 0.022359078750014305, 0.004154867958277464, -0.01981860212981701, 0.07707427442073822, -0.09615419805049896, -0.05025438219308853, -0.012814720161259174, 0.1083466112613678, 0.033220645040273666, 0.027706053107976913, 0.02809894271194935, -0.0056110345758497715, 0.03413591533899307, 0.03752489015460014, 0.015621466562151909, 0.07469586282968521, -0.003853463800624013, -0.03243357315659523, -0.013241875916719437, 0.013536425307393074, 0.027398109436035156, -0.001109386095777154, 0.036189448088407516, 0.0013048002729192376, 0.0367869958281517, -0.007771566044539213, -0.02460845559835434, -0.0014901328831911087, 0.07166128605604172, -0.024271667003631592, -0.0395553819835186, -0.005916551221162081, 0.04618696868419647, 0.011399803683161736, 0.036809783428907394, -0.03920406103134155, 0.009400950744748116, 0.04068983718752861, 0.010105034336447716, -0.04554574936628342, -0.025218622758984566, 0.05465638265013695, -0.04482119530439377, -0.04681159928441048, -0.04305410385131836, 0.05175508186221123, -0.0013017405290156603, -0.0009729975718073547, 0.004358190111815929, -0.008370156399905682, 0.01038815826177597, -0.020203446969389915, 0.05464167892932892, -0.021446755155920982, -0.010042494162917137, -0.006020053289830685, 0.014710955321788788, -0.016188425943255424, 0.04028262570500374, -0.013302701525390148, -0.08443653583526611, -0.058743491768836975, 0.03505841642618179, 0.06593751162290573, 0.029759639874100685, 0.0704042911529541, 0.02897576615214348, 0.0014979958068579435, -0.0065999398939311504, 0.022540101781487465, -0.005245905369520187, 0.014695586636662483, 0.013232053257524967, 0.006449453998357058, -0.014684145338833332, -0.047406911849975586, 0.05367811396718025, 0.08881649374961853, 0.06402647495269775, -0.02508886158466339, 0.0037916505243629217, -0.038193877786397934, -0.029171202331781387, 0.00851077027618885, -0.02342659793794155, 0.010511120781302452, -0.046310361474752426, -0.0019011961994692683, -0.07184570282697678, 0.012899955734610558, 0.01838122308254242, 0.004707892891019583, 0.014767112210392952, 0.019243715330958366, 0.025611812248826027, -0.023124106228351593, -0.08432075381278992, -0.000317775848088786, -0.003293209243565798, 0.001971443183720112, -0.002035732613876462, -0.0007662701536901295, -6.978445640563346e-33, -0.04076949134469032, -0.0032023857347667217, 0.03929414227604866, 0.010363197885453701, -0.004542754031717777, 0.014581385999917984, -0.02915172465145588, -0.06083466485142708, 0.03344415873289108, 0.027010543271899223, -0.004837774205952883, 0.02390195056796074, 0.0200162623077631, -0.01080406829714775, 0.007423287723213434, -0.024236712604761124, 0.027333641424775124, 0.02029760740697384, -0.016513077542185783, 0.019997844472527504, -0.028642961755394936, 0.03822774067521095, 0.0573866069316864, 0.013555912300944328, -0.07236818969249725, 0.03436919301748276, -0.007048614788800478, -0.023004790768027306, -0.08310946822166443, 0.02913069538772106, -0.038235075771808624, 0.0035103512927889824, -0.014521529898047447, -0.017274970188736916, -0.008052014745771885, -0.0035689789801836014, -0.08845344930887222, -0.0284331813454628, 0.0475430004298687, -0.001611460349522531, -0.02721949853003025, -0.05128658190369606, 0.001562044839374721, 0.060126662254333496, 0.012723173014819622, -0.05079970508813858, 0.0011069204192608595, 0.009375069290399551, -0.01568807289004326, -0.03251175954937935, -0.04097283259034157, 0.028637364506721497, 0.018474861979484558, 0.03063797391951084, 0.0589870922267437, -0.011874150484800339, -0.027402514591813087, 0.004804988391697407, -0.028625400736927986, -0.02286987006664276, 0.030547987669706345, 0.038607824593782425, 0.05508631840348244, 0.02239852398633957, 0.0388094037771225, 0.050069596618413925, 0.03723297268152237, -0.014174606651067734, -0.006343770772218704, 0.014952093362808228, 0.0038449163548648357, 0.06217861548066139, 0.04059121385216713, -0.015281516127288342, 0.03925195708870888, 0.024790098890662193, -0.10969432443380356, 0.01082793902605772, -0.022579429671168327, 0.04281482845544815, 0.037874363362789154, -0.003221893450245261, -0.013052063062787056, -0.005779935512691736, -0.024320757016539574, -0.04182906821370125, -0.06521608680486679, 0.007602533791214228, 0.030108723789453506, 0.02532663382589817, -0.055390603840351105, 0.02892070822417736, 0.022588033229112625, -0.01469494216144085, 0.053311724215745926, -0.038134943693876266, 0.02488883212208748, 0.006907064933329821, 0.01133558340370655, 0.06172265484929085, -0.01932484097778797, -0.07166814059019089, -0.010830219835042953, -0.008958924561738968, 0.033610984683036804, -0.023611389100551605, -0.015474367886781693, 0.0630844309926033, -0.07816066592931747, 0.016926351934671402, -0.0554664172232151, 0.03135297819972038, 0.041021574288606644, 0.04592195525765419, 0.01346136350184679, -0.014371058903634548, 0.01917225494980812, 0.04162081331014633, -0.0269997026771307, 0.014652056619524956, -0.03650031238794327, 0.005605184007436037, 0.0028517479076981544, 0.05125090107321739, -0.04522481933236122, -0.005965063814073801, -0.052674949169158936, -0.002726924605667591, -0.005221688188612461, -0.08862283080816269, -0.016823232173919678, 0.035277459770441055, 2.8343606572889257e-07, -0.0011149104684591293, 0.036627382040023804, 0.008735773153603077, 0.008826334029436111, -0.012716362252831459, -0.01380727905780077, 0.02909471094608307, 0.0028864997439086437, 0.020826978608965874, -0.012337951920926571, 0.004916626028716564, 0.02725827880203724, 0.041001297533512115, 0.005845847073942423, -0.02959529682993889, -0.015202522277832031, -0.02916371077299118, -0.003073146566748619, -0.03534333407878876, -0.03860350325703621, -0.04238804057240486, 0.10460881143808365, 0.038890428841114044, -0.013141374103724957, 0.05641172453761101, -0.012122378684580326, 0.034275490790605545, -0.006776231806725264, -0.005721536930650473, -0.014929107390344143, 0.047980327159166336, 0.020945679396390915, -0.03909512236714363, 0.025891778990626335, 0.006036424543708563, -0.0006620794301852584, -0.03145976737141609, 0.056952688843011856, -0.021890386939048767, -0.03158565238118172, 0.012421430088579655, -0.011371102184057236, 0.01180976815521717, -0.03312287479639053, 0.07222630083560944, 0.012051170691847801, -0.035465307533741, -0.028972366824746132, -0.004129582084715366, -0.04750039428472519, 0.051149096339941025, -0.013713495805859566, 0.06377419829368591, 0.025593744590878487, -0.01367380190640688, -0.037058375775814056, -0.00962786190211773, -0.04641421511769295, -0.011077217757701874, -0.0005057997186668217, -0.06918486952781677, -0.0364559106528759, -0.07238708436489105, -0.021507957950234413, 0.03568980470299721, -0.04765000194311142, -0.048508696258068085, 2.7681668253878566e-34, 0.05056406930088997, 0.002222189912572503, 0.007659154012799263, 0.05089303478598595, 0.031210819259285927, 0.0027000794652849436, -0.0011704202042892575, -0.011261709034442902, -0.0470212921500206, -0.07872317731380463, -0.033807069063186646], [0.08287755399942398, 0.028238877654075623, -0.03424882888793945, 0.01969696395099163, -0.03799762204289436, 0.0347498320043087, -0.0038253844249993563, -0.014717891812324524, -0.03642204403877258, -0.09742115437984467, -0.04396078363060951, 0.01622948795557022, -0.0008121802820824087, 0.03522581234574318, 0.01862419582903385, -0.045652467757463455, 0.07684330642223358, -0.027521586045622826, 0.009793806821107864, 0.02945139817893505, 0.006332429125905037, 0.03518572077155113, 0.008261501789093018, 0.02238159440457821, 0.018134301528334618, 0.002755632856860757, 0.022589482367038727, 0.00554804690182209, 0.0008026990690268576, 0.00885238777846098, 0.012958656996488571, 0.06167993322014809, -0.009111343882977962, -0.008767321705818176, 1.511850882707222e-06, -0.05640434846282005, -0.03657892346382141, 0.0002644092310220003, -0.031368304044008255, 0.033641062676906586, 0.05819190293550491, -0.02017906680703163, 0.004721328616142273, 0.03797585889697075, -0.0667710080742836, -0.0273024532943964, 0.0524013414978981, 0.029543878510594368, 0.07002197206020355, 0.10168547928333282, -0.007530460599809885, -0.042496442794799805, -0.005646808538585901, -0.01655944250524044, -0.05242399871349335, -0.06379969418048859, 0.04819204658269882, -0.005036633927375078, -0.02002761699259281, -0.0009420252754352987, -0.02364577353000641, 0.020304791629314423, -0.04759274423122406, 0.020406529307365417, 0.005552264861762524, -0.02509879134595394, -0.033567361533641815, -0.0021659620106220245, -0.03326546773314476, 0.04555308818817139, -0.06257203966379166, -0.0291878804564476, -0.0037803167942911386, -0.04053841531276703, -0.024074435234069824, 0.004156412091106176, -0.02588847279548645, -0.02802898921072483, 0.023861059918999672, 0.0010094937169924378, 0.012213007546961308, -0.03023233264684677, 0.005377983674407005, 0.016347510740160942, -0.04481648653745651, 0.06302263587713242, 0.018673747777938843, -0.006003689952194691, -0.019731618463993073, -0.010662409476935863, -0.028894778341054916, -0.03796852380037308, 0.03527143970131874, 0.009029981680214405, -0.0002871087926905602, 0.01316300593316555, -0.061481837183237076, -0.008944799192249775, 0.006688989698886871, -0.04233207553625107, 0.07677570730447769, 0.016667857766151428, 0.0022924889344722033, 0.05279896780848503, -0.03901372849941254, -0.01675577089190483, 0.012494157068431377, 0.039564795792102814, -0.03537079691886902, 0.0166280847042799, -0.040862180292606354, -0.0037907538935542107, -0.09279612451791763, 0.019465353339910507, -0.05250721052289009, -0.04630693420767784, -0.04513015225529671, 0.02500656619668007, 0.03976145759224892, -0.051561981439590454, 0.02024843543767929, -0.002327731577679515, -0.03569804131984711, -0.01700124889612198, -0.0017509767785668373, 0.052211642265319824, -0.0335724800825119, 0.04924389347434044, -0.014768347144126892, -0.046581774950027466, -0.0035821590572595596, -0.012530951760709286, 0.055605001747608185, -0.022542385384440422, 0.026236949488520622, 0.04907061159610748, 0.047483205795288086, -0.04547308012843132, -0.034781619906425476, 0.01863754913210869, -0.025534270331263542, -0.06230689585208893, -0.019223380833864212, -0.028958957642316818, 0.018913403153419495, 0.016199730336666107, 0.028929220512509346, -0.03540784493088722, -0.011924703605473042, -0.0069319093599915504, -0.08392713963985443, 0.09070420265197754, -0.014403470791876316, 0.02161792851984501, -0.006885032169520855, -0.04345112293958664, 0.05264491215348244, 0.0793776661157608, 0.03288527578115463, -0.006873374339193106, 0.0745762288570404, -0.043748922646045685, -0.016014866530895233, 0.04402559995651245, -0.016907835379242897, -0.005799064878374338, 0.004961036611348391, 0.005390113685280085, 0.0029748829547315836, 0.00730549031868577, -0.016283292323350906, 0.016936147585511208, -0.011413236148655415, 0.02756439708173275, 0.025614570826292038, 0.049426645040512085, 0.07221502810716629, 0.0187673419713974, 0.03415863960981369, -0.016819143667817116, -0.010805265046656132, -0.005939347203820944, -0.06297843903303146, -0.02426717057824135, -0.05480530485510826, -0.02000334858894348, 0.005797602701932192, 0.06198343262076378, -0.010105984285473824, -0.035924334079027176, -0.017472343519330025, 0.014357593841850758, 0.010048707015812397, -0.01815428026020527, 0.05820680037140846, 0.040156204253435135, -0.01136325765401125, 0.018393676728010178, 0.04453282430768013, 0.003214701544493437, 0.030775094404816628, -0.047110144048929214, -0.021598711609840393, 0.07917183637619019, 0.047966402024030685, -0.04223024100065231, -0.026715798303484917, 0.014364862814545631, 0.007167812902480364, 0.015539156273007393, 0.06808121502399445, -0.015182307921350002, 0.013927220366895199, -0.034792955964803696, -0.018302030861377716, 0.014181727543473244, 0.030906785279512405, 0.00695929815992713, 0.00884187500923872, 0.018557438626885414, 0.034342002123594284, 0.0059197708033025265, -0.06654644012451172, 0.05117269977927208, -0.005730445496737957, -0.037680789828300476, 0.02688971534371376, -0.00895844679325819, 0.008563975803554058, 0.008973445743322372, -0.017751140519976616, 0.011230241507291794, 0.03102198615670204, -0.008653989061713219, 0.04058476537466049, -0.012620383873581886, 0.010700654238462448, -0.002469154540449381, 0.03404177352786064, 0.009204170666635036, 0.042699798941612244, 0.019286610186100006, -0.051367778331041336, -0.037429459393024445, 0.023826036602258682, -0.0012380817206576467, 0.05408097803592682, -0.014029118232429028, 0.05561463534832001, -0.01836378686130047, 0.04024201259016991, -0.0466272346675396, 0.0057770321145653725, 0.047559190541505814, -0.0009801972191780806, -0.02678133174777031, -0.015659179538488388, 0.0028839465230703354, -0.008002745918929577, -0.022936630994081497, 0.011724621057510376, -0.02045278809964657, 0.0030769703444093466, 0.03513740748167038, 0.004003561567515135, -0.03830639645457268, -0.0065796636044979095, -0.04634944722056389, 0.03140849620103836, -0.0374850332736969, 0.02385644055902958, 0.04570765793323517, 0.03785933926701546, -0.03570656105875969, -0.03748338669538498, 0.02410598285496235, 0.033212363719940186, 0.021507736295461655, 0.03641919046640396, 0.0064513287506997585, 0.003986299503594637, 0.006564858835190535, -0.04510675370693207, -0.0303956251591444, 0.014118125662207603, 0.03640316426753998, -0.021706683561205864, -0.037961460649967194, -0.09565360844135284, 0.01079112570732832, -0.017471471801400185, 0.05189760774374008, 0.03545023873448372, -0.04751458391547203, -0.032059017568826675, -0.011103978380560875, 0.023920491337776184, 0.05921640619635582, -0.0035836573224514723, 0.026091359555721283, 0.031413134187459946, -0.00882729422301054, -0.01874341070652008, -0.002652418101206422, -0.05770208686590195, -0.06437907367944717, -0.05221322923898697, -0.002619726350530982, 0.017218120396137238, 0.050503041595220566, -0.014811025001108646, -0.015800565481185913, -0.04509451612830162, 0.04348338022828102, -0.03461354225873947, -0.014631937257945538, 0.002094498835504055, -0.011338341981172562, -0.007142300251871347, 0.05819198489189148, 0.0054862224496901035, -0.03832497075200081, 0.02913809008896351, -0.023222923278808594, 0.025176655501127243, -0.07295992225408554, -0.01866290532052517, -0.017423801124095917, 0.04474976658821106, 0.006402202881872654, 0.026307817548513412, 0.016721228137612343, -0.03167946636676788, -0.015785738825798035, -0.01407838985323906, 0.03480689972639084, -0.019740398973226547, 0.021261245012283325, -0.02689369022846222, 0.012165859341621399, -0.011302306316792965, -0.009194067679345608, -0.009890005923807621, -0.07245809584856033, -0.04053083062171936, 0.006989739835262299, 0.03387880325317383, -0.002349067246541381, -0.050362128764390945, 0.04842688888311386, -0.021915804594755173, 0.04082513600587845, -0.015005935914814472, -0.004169382154941559, -0.04782935976982117, 0.01818244159221649, 0.06701978296041489, 0.024927109479904175, 0.035275574773550034, 0.027110693976283073, 0.0062857987359166145, 0.0036427343729883432, 0.016362570226192474, 3.973640559706837e-06, -0.0063524870201945305, 0.018358619883656502, 0.03814812749624252, -0.07226183265447617, 0.002466404577717185, 0.01958966627717018, 0.044287651777267456, -0.057227469980716705, -0.0422680489718914, 0.11113371700048447, 0.03735840320587158, -0.004976313095539808, 0.018697421997785568, 0.03857862576842308, -0.005420480854809284, 0.01626596227288246, 0.03749542683362961, -0.04605264961719513, -0.03516777232289314, 0.011844577267765999, 0.018661346286535263, -0.022768737748265266, -0.011443317867815495, -0.013503766618669033, -0.11326374113559723, 0.037177443504333496, 0.019961850717663765, -0.017896052449941635, -0.023793159052729607, -0.05508846044540405, 0.01659892499446869, -0.02796865627169609, 0.0246905367821455, -0.018933888524770737, -0.019427025690674782, -0.017489979043602943, 0.01901371031999588, -0.010265974327921867, -0.08424337208271027, 0.0054766107350587845, -0.04017539694905281, 0.058153923600912094, 0.07386363297700882, 0.007803621236234903, -0.008319297805428505, 0.021476801484823227, 0.07503931224346161, -0.05038994550704956, 0.039666611701250076, -0.04973065108060837, 0.03154657781124115, -0.007634038105607033, -0.02241239696741104, 0.008814428001642227, 0.004802210256457329, -0.024949537590146065, -0.011662726290524006, -0.015521431341767311, 0.03472348302602768, -0.006746423430740833, -0.022436432540416718, 0.0006840847199782729, 0.012797880917787552, -0.003580271964892745, 0.057203225791454315, 0.02077794261276722, -0.032892875373363495, -0.018262067809700966, 0.0013670780463144183, -0.020254656672477722, -0.03005785495042801, -0.04618004709482193, -0.0008640466840006411, -0.045059964060783386, -0.019467944279313087, -0.0677991732954979, -0.013877028599381447, 0.059645891189575195, 0.04387219622731209, 0.02725983038544655, 0.037285216152668, 0.05907582864165306, 0.002776126377284527, -0.02852870337665081, 0.03700399771332741, -0.02146148681640625, -0.008037319406867027, -0.038806673139333725, -0.028757954016327858, 0.061588432639837265, 0.018915878608822823, -0.02937883511185646, -0.00217105308547616, -0.05846374109387398, -0.07730208337306976, 0.03677612170577049, 0.05503414571285248, 0.026367895305156708, 0.036596640944480896, -0.05053740367293358, -0.07428927719593048, 0.00655621662735939, 0.028558235615491867, -0.04647090658545494, 0.02562985196709633, 0.0031102041248232126, -0.03759252279996872, -0.03441428020596504, 0.03284454718232155, -0.025797974318265915, 0.0497727207839489, 0.05682032182812691, 0.018876126036047935, 0.014978793449699879, -0.0904400423169136, 0.008612788282334805, 0.03569089621305466, 0.08594727516174316, 0.007875103503465652, -0.04599776118993759, -0.028738560155034065, 0.027678564190864563, -0.013356500305235386, -0.017328618094325066, -0.01923736184835434, -0.012282605282962322, 0.030258361250162125, 0.012952923774719238, -0.06008584797382355, -0.001895167981274426, -0.032702382653951645, 0.01626645028591156, -0.07524025440216064, 0.019492721185088158, 0.031492896378040314, 0.00804526824504137, 0.025008538737893105, 0.0034152192529290915, -0.013474834151566029, 0.047848284244537354, 0.015137283131480217, 0.03876654431223869, -0.04217774420976639, 0.009638061746954918, 0.0033364510163664818, 0.022466745227575302, -0.07874786108732224, 0.058685146272182465, -0.054558273404836655, -0.06994152069091797, -0.05289728194475174, 0.046857114881277084, 0.009751265868544579, -0.053082965314388275, 0.052356377243995667, -0.011580042541027069, -0.023229317739605904, -0.01942589320242405, 0.0023186414036899805, 0.013956933282315731, 0.02843857929110527, -0.005023447796702385, -0.007162111811339855, -0.03432422876358032, 0.007879064418375492, 0.03605261817574501, -0.021928755566477776, 0.0856996551156044, 0.012316720560193062, -0.0352054238319397, -0.01743650622665882, -0.00650444021448493, -0.035352256149053574, 0.013064419850707054, -0.016130546107888222, 0.04912488907575607, -0.016733404248952866, -0.05069759860634804, -0.01728697307407856, -0.011670731008052826, 0.02318858541548252, -0.018667543306946754, 0.017871852964162827, 0.05966220796108246, 0.007852843031287193, -0.004069914575666189, -0.03245887532830238, -0.013078615069389343, 0.003333630273118615, -0.006359144113957882, 0.014223369769752026, -6.401079549637902e-33, -0.06516104936599731, 0.02265326865017414, -0.004684019833803177, 0.0019227780867367983, -0.006829618476331234, -0.04124319925904274, 0.016925761476159096, -0.005917208269238472, -0.027107305824756622, -0.025321578606963158, -0.036781568080186844, 0.004011525772511959, 0.032930657267570496, -0.0027461424469947815, -0.0017759562470018864, -0.005938769318163395, 0.052771519869565964, -0.003116224892437458, -0.033145610243082047, 0.0006375729572027922, 0.026103723794221878, 0.03732083737850189, 0.07987553626298904, -0.05042373016476631, -0.016367662698030472, 0.047605209052562714, -0.022525088861584663, 0.020506026223301888, -0.012539606541395187, 0.05007420480251312, -0.02428370900452137, 0.029902059584856033, 0.03442281112074852, -0.017700450494885445, -0.03615939989686012, 0.025276675820350647, -0.09399218112230301, -0.012579504400491714, 0.06441686302423477, 0.03671620413661003, 0.016895238310098648, -0.006002222187817097, 0.06725185364484787, -0.009082894772291183, -0.040227025747299194, 0.023092247545719147, 0.012209397740662098, -0.04737492650747299, -0.0132516548037529, -0.004851743578910828, -0.0893501341342926, 0.026039356365799904, 0.01348081324249506, -0.009398136287927628, 0.02438969537615776, 0.0011268696980550885, -0.018912307918071747, 0.00534797040745616, -0.012198385782539845, -0.007609691470861435, 0.07360487431287766, 0.08587207645177841, 0.02691251039505005, 0.010690330527722836, -0.025382565334439278, 0.02706320770084858, -0.004944654647260904, 0.00954817421734333, 0.030316969379782677, -0.002887811278924346, 0.055344097316265106, 0.04497909918427467, 0.012411349453032017, 0.01341817807406187, 0.04627925902605057, -0.04559990018606186, -0.04454183951020241, -0.0219466183334589, -0.018949931487441063, 0.03580665960907936, 0.0286688432097435, 0.010507689788937569, -0.005607232917100191, -0.018811475485563278, -0.07722072303295135, -0.014207568019628525, -0.04680118337273598, -0.0043657273054122925, 0.032337721437215805, 0.019319942221045494, -0.04209281876683235, 0.05399240180850029, 0.022847793996334076, -0.03835979104042053, -0.000542710826266557, -0.028668096289038658, 0.006470288150012493, -0.01355182658880949, -0.03297058120369911, 0.034240905195474625, -0.020486116409301758, -0.05501843988895416, 0.00295805255882442, -0.01468309573829174, -0.0010792686371132731, -0.0394231453537941, 0.03417783975601196, 0.01695903018116951, -0.07698678225278854, 0.014350985176861286, 0.002751880092546344, 0.02775527909398079, 0.022428693249821663, 0.05421099066734314, 0.029399579390883446, 0.005518326070159674, -0.004399958066642284, 0.04944639652967453, 0.043541841208934784, -0.043765828013420105, -0.004166664555668831, 0.0062850844115018845, 0.04913976415991783, 0.02978825755417347, -0.03136059641838074, -0.00773050170391798, -0.011360774748027325, 0.0005965486634522676, 0.003202992957085371, -0.04122410714626312, -0.017299849539995193, 0.005817130208015442, 2.371355520836005e-07, 0.0772157832980156, 0.07532753795385361, -0.0005434153717942536, -0.033353690057992935, 0.02096967212855816, 0.00933884084224701, -0.011970140039920807, 0.032492056488990784, 0.03115391731262207, 0.029518406838178635, 0.0143965445458889, 0.005375486332923174, 0.020883258432149887, -0.02390550822019577, -0.037739768624305725, 0.0008421483216807246, -0.03155647963285446, 0.03730443865060806, -0.06415347009897232, -0.01618904434144497, 0.016112124547362328, 0.11057113111019135, -0.010397124104201794, 0.00397754879668355, -0.032310161739587784, -0.07657507061958313, 0.026486525312066078, -0.03729735314846039, 0.0030738438945263624, 0.05631522834300995, 0.015237235464155674, 0.04449201747775078, 0.0010855008149519563, 0.020519984886050224, -0.03610197454690933, 0.05391364544630051, -0.015213275328278542, 0.06799738109111786, -0.04787900298833847, 0.013798892498016357, -0.012044988572597504, 0.0015384892467409372, 0.004144099075347185, -0.03086582012474537, 0.08981531113386154, -0.08884642273187637, -0.017404837533831596, -0.057735785841941833, -0.015862446278333664, -0.010779640637338161, 0.004704912193119526, -0.02937084063887596, 0.0022584302350878716, 0.029370779171586037, 0.05817873775959015, 0.01277315616607666, -0.05703213810920715, -0.1147514134645462, 0.017280427739024162, 0.050187479704618454, -0.00735651096329093, -0.02786095254123211, -0.0222318097949028, -0.017925003543496132, 0.0653228759765625, -0.057479128241539, 0.015753960236907005, 2.03743729258849e-34, 0.0373622365295887, 0.017793456092476845, 0.019937362521886826, 0.005460349842905998, 0.02907339669764042, -0.005128436256200075, 0.002568962285295129, 0.02893889509141445, 0.002947113011032343, -0.13457635045051575, -0.012935674749314785], [-0.007974080741405487, 0.07692158222198486, -0.002628743415698409, 0.012730348855257034, 0.014568055048584938, 0.023137684911489487, -0.016642602160573006, -0.004346096888184547, -0.055120863020420074, 0.016572898253798485, 0.016658702865242958, 0.034264255315065384, -0.022027162835001945, 0.04940192401409149, 0.009662340395152569, -0.09738240391016006, 0.030732976272702217, -0.03060719557106495, -0.0367174856364727, -0.02402579039335251, -0.009224211797118187, 0.0011683206539601088, 0.0353706069290638, 0.013504398986697197, -0.02002633921802044, 0.0021204231306910515, -0.06843933463096619, 0.03396955877542496, 0.04005322977900505, -0.013879905454814434, -0.0042813788168132305, 0.027358606457710266, 0.04167664051055908, 0.047934334725141525, 1.8399291548121255e-06, 0.006231431383639574, -0.07732831686735153, 0.04955374449491501, -0.056807465851306915, 0.06740598380565643, 0.055130742490291595, -0.01822056993842125, 0.009610073640942574, 0.0257156640291214, 0.027957241982221603, -0.011516517028212547, 0.038294386118650436, 0.07158497720956802, 0.04995901137590408, 0.05588661506772041, 0.006098742131143808, -0.010088476352393627, 0.015016265213489532, -0.039690051227808, 0.002212188206613064, -0.05973654240369797, 0.01571383886039257, 0.0020399463828653097, -0.0009714493644423783, -0.0242199394851923, -0.03460071608424187, -0.003542616730555892, 0.049685582518577576, 0.04832478612661362, 0.019759122282266617, 0.02217012271285057, -0.017193783074617386, 0.02562701143324375, -0.027654176577925682, -0.01154299546033144, -0.011518743820488453, -0.023126062005758286, 0.013087846338748932, -0.02220793254673481, -0.02783888764679432, -0.02020205743610859, -0.02440633252263069, -0.034631796181201935, -0.028270967304706573, 0.005735393147915602, -0.05180156230926514, -0.07241711765527725, 0.005539183970540762, -0.02691209688782692, -0.06710116565227509, 0.0729341059923172, 0.06100202724337578, -0.0004847587551921606, -0.02345682866871357, -0.020621711388230324, -0.05973045900464058, -0.002455188427120447, 0.015329851768910885, -0.006873553618788719, -0.03813115134835243, -0.014354065991938114, -0.06326699256896973, 0.008407136425375938, 0.015845440328121185, 0.0313866063952446, 0.09054379910230637, 0.01870657131075859, 0.02069438248872757, 0.028014859184622765, -0.008035541512072086, 0.033788371831178665, -0.09466426074504852, 0.01656312122941017, -0.022291701287031174, -0.023181084543466568, -0.0758848488330841, -0.008910010568797588, -0.08166354894638062, -0.0004551261372398585, 0.009251505136489868, -0.016614776104688644, -0.04041392356157303, 0.03267084062099457, 0.029413973912596703, -0.03195615112781525, 0.046249281615018845, 0.01562619023025036, -0.05821651965379715, -0.012013180181384087, -0.008139224722981453, 0.031859129667282104, 1.833053829614073e-05, -0.017928289249539375, 0.026681629940867424, -0.009540650062263012, -0.007358838804066181, -0.04110129177570343, 0.011093695648014545, -0.040663447231054306, 0.030319103971123695, -0.0048057399690151215, 0.029551150277256966, -0.037580426782369614, 0.002238973742350936, -0.005607144441455603, 0.0002820835798047483, -0.04199260473251343, -0.028384296223521233, -0.010011797770857811, -0.030134927481412888, -0.013667494989931583, 0.008483602665364742, -0.027666687965393066, -0.0406426265835762, -0.012358642183244228, -0.06967101991176605, 0.05146335810422897, 0.0003142623754683882, -0.01024776790291071, 0.0349934883415699, 0.0009773282799869776, -0.013039861805737019, 0.044801197946071625, 0.026005662977695465, 0.013784084469079971, -0.009942648932337761, -0.02471754141151905, 0.006205108482390642, 0.016182560473680496, -0.008112888783216476, 0.041729286313056946, -0.005285291932523251, 0.013845612294971943, 0.06226802244782448, -0.00912176538258791, 0.00965789332985878, -0.019856300204992294, -0.01290647592395544, -0.03700854629278183, 0.055547989904880524, -0.01712953671813011, 0.03158186003565788, 0.03464600071310997, 0.013867534697055817, -0.008144063875079155, 0.017270470038056374, -0.048840466886758804, -0.07377996295690536, 0.027641549706459045, -0.04508385434746742, -0.005304987542331219, -0.017870813608169556, 0.046354491263628006, -0.04252535477280617, -0.014144381508231163, 0.016844769939780235, 0.023174433037638664, -0.06020420789718628, 0.00852090772241354, 0.05682128295302391, -0.05537288263440132, -0.03264571353793144, 0.012189766392111778, 0.02547767385840416, -0.05405261740088463, -0.05306084454059601, -0.07437384128570557, 0.025569306686520576, -0.01549242902547121, 0.023677188903093338, -0.04105376452207565, -0.10772843658924103, 0.022624896839261055, 0.027701376006007195, 0.009967288933694363, 0.07652536779642105, 0.02836059220135212, 0.05034957826137543, 0.007192141376435757, -0.03980126231908798, 0.019506849348545074, 0.03207468241453171, 0.03451021388173103, 0.015124890953302383, 0.01506752148270607, 0.004661895334720612, -0.025301214307546616, 0.0069654849357903, 0.0021575165446847677, -0.006183291785418987, 0.0033393967896699905, 0.012890661135315895, -0.003066723933443427, 0.038740240037441254, 0.0031256836373358965, -0.008578895591199398, -0.05726013705134392, 0.02029344066977501, 0.009473180398344994, -0.007648150902241468, -0.017016122117638588, 0.0005144040333107114, 0.060149505734443665, 0.01645629294216633, -0.028791163116693497, 0.01392486784607172, 0.010765066370368004, -0.050692588090896606, -0.05543502792716026, 0.009887189604341984, -0.0009702210081741214, 0.0809309110045433, -0.009244772605597973, 0.044632889330387115, -0.014761832542717457, 0.005624260287731886, 0.02481221966445446, 0.0017901784740388393, 0.02068498358130455, -0.07393552362918854, 0.038032133132219315, 0.030825411900877953, 0.05358113721013069, -0.056637462228536606, -0.021233251318335533, -0.0034430029336363077, 0.04021293669939041, -0.005781249143183231, 0.023442942649126053, 0.005612604320049286, -0.013071105815470219, -0.027508361265063286, -0.04907073453068733, 0.03772639110684395, -0.02348356693983078, 0.031317710876464844, 0.03696789965033531, 0.0039044590666890144, -0.025989659130573273, 0.03221549093723297, 0.01603531464934349, 0.021486392244696617, -0.029849687591195107, -0.008312532678246498, -0.029567616060376167, 0.021781183779239655, -0.06441191583871841, -0.03300320357084274, 0.0025788163766264915, 0.00893083680421114, 0.031689003109931946, -0.005641891621053219, 0.02122192643582821, -0.06430801749229431, 0.050340957939624786, -0.0030631162226200104, 0.03697710111737251, -0.02610202133655548, -0.040359195321798325, 0.0033566912170499563, -0.03518805280327797, -0.005966911558061838, 0.032825253903865814, -0.020219888538122177, 0.03565589338541031, 0.021950561553239822, 0.014254693873226643, -0.014194573275744915, 0.017194945365190506, -0.037017423659563065, -0.09863525629043579, 0.00036627944791689515, 0.0061429268680512905, -0.01038779690861702, -0.01318273227661848, -0.0541752465069294, -0.015112712979316711, -0.06996676325798035, 0.005921069998294115, 0.0029598530381917953, -0.034226302057504654, -0.02501654624938965, 0.01718234270811081, -0.01972527801990509, -0.024432437494397163, -0.021801771596074104, -0.03349354490637779, -2.99496459774673e-05, -0.011895542033016682, 0.018132822588086128, -0.03765344247221947, -0.03297213837504387, -0.01737162470817566, 0.026202337816357613, 0.04056365042924881, 0.03568585589528084, 0.042459093034267426, 0.02408938854932785, -0.04503263533115387, -0.01142630260437727, 0.019534213468432426, 0.003282881574705243, -0.03401697799563408, -0.018634116277098656, 0.022345293313264847, 0.01613534428179264, -0.06027728319168091, 0.012706798501312733, 0.03303958475589752, 0.019703593105077744, 0.031453508883714676, -0.03028087131679058, -0.06540362536907196, -0.026458824053406715, 0.001689159544184804, -0.004770110361278057, 0.05913332849740982, -0.019208166748285294, -0.022043785080313683, -0.02932894229888916, -0.01111465785652399, 0.059442367404699326, 0.017493294551968575, 0.018739495426416397, -0.06659197062253952, 0.03179338574409485, 0.022998910397291183, 0.07725972682237625, -0.02452249638736248, -0.09054473042488098, 0.04543951153755188, -0.015124150551855564, -0.005459594540297985, -0.0270399060100317, 0.019970344379544258, 0.047264464199543, -0.06160067021846771, 0.019473768770694733, 0.06742176413536072, -0.017644204199314117, -0.11715636402368546, -0.018304482102394104, 0.011816615238785744, 0.09031445533037186, 0.01804232969880104, 0.03166104853153229, -0.0013675722293555737, 0.014050331898033619, 0.0323922373354435, 0.02220088616013527, -0.01721450872719288, 0.008862284943461418, 0.0033152238465845585, -0.06886289268732071, 0.07779915630817413, -0.008847441524267197, -0.06524744629859924, -0.04929636791348457, -0.035089269280433655, 0.011696196161210537, -0.042590294033288956, -0.01679564081132412, 0.043931908905506134, -0.041956834495067596, 0.005967437289655209, 0.020145654678344727, 0.030446002259850502, -0.010362808592617512, -0.00973831582814455, -0.04330823943018913, 0.02849135361611843, -0.001068632467649877, -0.015001337975263596, -0.02340741828083992, -0.006008641794323921, 0.017948037013411522, -0.022347278892993927, 0.0027250603307038546, 0.0033594451379030943, 0.02513311058282852, 0.021866237744688988, -0.026198096573352814, 0.0025377743877470493, 0.04876609146595001, 0.010689196176826954, -0.009858330711722374, 0.02024945802986622, 0.07922875136137009, -0.023586954921483994, -0.014082049019634724, -0.024594204500317574, -0.0761788934469223, -0.019006457179784775, 0.05010141432285309, 0.012025514617562294, -0.014123077504336834, -0.05400847643613815, -0.013973301276564598, -0.02935142256319523, 0.0335024893283844, 0.03225131332874298, 0.0020551092457026243, -0.022286752238869667, -0.048145171254873276, 0.007769073825329542, 0.035933081060647964, 0.04948888719081879, 0.0945882499217987, 0.006276426371186972, 0.0027853993233293295, 0.01180870458483696, 0.03649159520864487, -0.0564037524163723, 0.030375780537724495, -0.010826840996742249, -0.0028796144761145115, -0.09458021074533463, -0.014462395571172237, 0.03307989984750748, 0.008861829526722431, -0.038827139884233475, -0.03929464891552925, -0.06742091476917267, -0.0434863455593586, 0.017885155975818634, 0.07079067081212997, 0.043063320219516754, 0.03378593176603317, 0.00025811107479967177, -0.032312676310539246, 0.037990909069776535, 0.009280736558139324, 0.01310067530721426, 0.03432246670126915, 0.011413069441914558, -0.04088182374835014, -0.042761363089084625, -0.013347091153264046, -0.026495901867747307, 0.027247941121459007, 0.022441811859607697, -0.020385293290019035, -0.0029340300243347883, -0.045489393174648285, 0.0034912084229290485, 0.03429894149303436, 0.0405535027384758, 0.01984146051108837, 0.003647350473329425, -0.01890772394835949, 0.023800445720553398, -0.009884417988359928, 0.047905780375003815, -0.0428297333419323, 0.03869323804974556, 0.004453980829566717, 0.01840209774672985, -0.026686962693929672, -0.036225825548172, 0.07226402312517166, 0.00036883592838421464, -0.07209458947181702, -0.016015000641345978, -0.014601913280785084, 0.033068057149648666, 0.04647908732295036, -0.023563407361507416, 0.025894511491060257, -0.0028830328956246376, -0.007019301876425743, -0.0011171082733199, -0.017762837931513786, 0.041515737771987915, 0.012479499913752079, 0.0011047226144000888, -0.02942994236946106, 0.03425181657075882, -0.06582798808813095, -0.08665720373392105, -0.06593815982341766, 0.07118061184883118, 0.05077924579381943, 0.07483427226543427, 0.058144357055425644, -0.05175375938415527, -0.048914432525634766, -0.03228720650076866, 0.022927606478333473, 0.03712731972336769, 0.05142854526638985, 0.04026411101222038, 0.004671512171626091, 0.002124983351677656, 0.02204088121652603, 0.04512467980384827, 0.062309630215168, 0.10157019644975662, -0.019507156684994698, -0.006461900193244219, 0.00423090485855937, -0.013923976570367813, 0.025635236874222755, -0.03540291264653206, -0.0017829224234446883, 0.006079191341996193, 0.0030221580527722836, -0.05568515509366989, 0.019285067915916443, 0.03402028977870941, 0.02587277814745903, -0.037251003086566925, 0.05590061843395233, 0.018722930923104286, -0.03989847004413605, -0.03293285146355629, 0.028645873069763184, -0.051861099898815155, -0.01820644736289978, 0.009940354153513908, 0.06226609647274017, -6.0272032857833035e-33, -0.006120895501226187, 0.06618383526802063, 0.006293207872658968, 0.02047925442457199, -0.04619491100311279, -0.022688936442136765, 0.0034903553314507008, -0.0010974258184432983, 0.05528087168931961, -0.003784383647143841, -0.02771436795592308, 0.0070922053419053555, 0.002658448414877057, -0.0005759820924140513, 0.023101046681404114, -0.002131131710484624, 0.0773601084947586, -0.0052650789730250835, -0.012754730880260468, 0.05477123335003853, -0.0041166506707668304, 0.0382482185959816, 0.03718091920018196, -0.05722690001130104, -0.07951746135950089, 0.0525769367814064, -0.039074745029211044, 0.004605114459991455, -0.03082328662276268, 0.05851377546787262, -0.025655003264546394, -0.006293445359915495, -0.04691888391971588, -0.06107313185930252, -0.0014548652106896043, -0.06361659616231918, -0.036595892161130905, 0.010400093160569668, -0.01630321890115738, -0.0020709980744868517, 0.03114272840321064, -0.05671902000904083, 0.003221335355192423, 0.03159157931804657, -0.023245643824338913, 0.0037045329809188843, 0.019630013033747673, -0.006185526959598064, 0.0010329631622880697, -0.012487984262406826, -0.008729781955480576, 0.039698097854852676, -0.002083041938021779, 0.047962039709091187, 0.010281452909111977, -0.001550497836433351, -0.025881536304950714, -0.0017998749390244484, 0.008202352561056614, -0.032197289168834686, 0.061984892934560776, 0.008103162050247192, -0.010788247920572758, 0.005842177197337151, -0.002640180755406618, 0.034983474761247635, -0.016881169751286507, -0.00976419448852539, -0.007281760219484568, -0.013101086020469666, 0.0011027089785784483, 0.10898879915475845, 0.040186479687690735, 0.03638240694999695, 0.046194545924663544, -0.01960149221122265, -0.037301331758499146, 0.025428585708141327, -0.0072728111408650875, 0.03553882986307144, 0.02391778491437435, 0.004730741959065199, -0.013673306442797184, 0.017199784517288208, -0.019225923344492912, -0.05603679642081261, -0.03378811106085777, -0.01657366380095482, 0.06216983124613762, -0.000881761487107724, -0.04453197121620178, 0.01594548486173153, 0.025038961321115494, -0.028233956545591354, 0.03703642636537552, 0.009178309701383114, -0.033411454409360886, 0.023475514724850655, 0.006965866778045893, 0.05681208148598671, 0.02919226512312889, -0.08752308040857315, 0.003157448023557663, 0.006769864354282618, 0.030983952805399895, -0.018617188557982445, 0.005219186190515757, 0.053017694503068924, -0.05171126127243042, 0.03421701863408089, -0.02450508251786232, -0.01513613946735859, -0.00485602393746376, 0.013053860515356064, 0.005389375612139702, -0.018464094027876854, 0.03435628488659859, -0.0006007495103403926, -0.023611657321453094, -0.0036582225002348423, -0.03572123497724533, 0.018496662378311157, 0.050807856023311615, 0.036457598209381104, 0.02720392681658268, -0.0026454117614775896, -0.024381237104535103, -0.04026445001363754, 0.026340315118432045, -0.03744462504982948, -0.02661792002618313, 0.03727152571082115, 2.520755799650942e-07, -0.015869906172156334, 0.043490611016750336, 0.0012786650331690907, -0.04653095453977585, 0.024980295449495316, 0.023338932543992996, -0.029770221561193466, 0.046081267297267914, 0.03336067497730255, 0.010124508291482925, 0.028748488053679466, 0.01984385773539543, 0.040493421256542206, -0.029393251985311508, -0.02405380643904209, -0.02333880215883255, -0.021252531558275223, 0.014141418039798737, -0.06829477101564407, 0.009298103861510754, 0.006565439514815807, 0.10435079038143158, 0.0461362861096859, -0.020355768501758575, 0.0309599656611681, 0.01594192162156105, 0.022166214883327484, -0.05317675322294235, 0.035242412239313126, -0.002782760886475444, 0.03260648623108864, 0.03664610534906387, -0.041084323078393936, 0.017963234335184097, 0.02373860590159893, 0.025580396875739098, -0.00032866536639630795, 0.07989665865898132, 0.004268661607056856, -0.012761621735990047, 0.03952953219413757, 0.0336681604385376, -0.004925793968141079, -0.07219935953617096, 0.0838397815823555, -0.010906280018389225, 0.005140717141330242, -0.044279977679252625, -0.05042858421802521, -0.06827657669782639, 0.03272367641329765, -0.017921822145581245, 0.02732435241341591, 0.009692590683698654, -0.021279074251651764, -0.050600308924913406, -0.01618283800780773, -0.06751875579357147, 0.06711796671152115, 0.03811093792319298, -0.05262671038508415, -0.0552472248673439, -0.04451940581202507, -0.00582575798034668, 0.06790370494127274, -0.03580056503415108, -0.015072920359671116, 2.5561354247371037e-34, 0.05505741760134697, -0.034232158213853836, 0.05930278077721596, -0.029361221939325333, 0.008650708012282848, 0.010497409850358963, 0.05310368165373802, -0.02269286848604679, -0.004365914035588503, -0.10912300646305084, -0.033159464597702026]]\n"
     ]
    }
   ],
   "source": [
    "chunks_embeddings = huggingface_embedding.embed_documents(chunks)\n",
    "print(f\"First 5 embeddings for the chunks:\\n{chunks_embeddings[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53145cb3",
   "metadata": {},
   "source": [
    "## Task 4 - Create and configure vector databases to store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63fda3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [str(i) for i in range(0, len(chunks))]\n",
    "vectordb = Chroma.from_texts(chunks, huggingface_embedding, ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e6b3e4",
   "metadata": {},
   "source": [
    "## Task 5 - Develop a retriever to fetch document segments based on queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8918b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retrieved: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='isting weight matrices. As detailed in Section 4.2, LoRA is applied to the query\\nand value matrices in most experiments. The number of trainable parameters\\nis determined by the rank r and the shape of the original weight matrices:\\n|Θ| = 2 × LLoRA × dmodel × r, where LLoRA represents the number of weight\\nmatrices to which LoRA is applied.'),\n",
       " Document(metadata={}, page_content='descent. A variant of this is fine-tuning only select layers, while freezing the\\nrest. One such baseline from prior work on GPT-2 updates only the last two\\nlayers (denoted as FTTop2).\\nBitFit is another baseline in which only the bias parameters are updated,\\nwhile all other parameters remain frozen. This method has gained attention,\\nincluding in recent studies [?].'),\n",
       " Document(metadata={}, page_content='to SQL (NL2SQL). Each downstream task is represented as a training set of\\ncontext-output pairs:\\nZ = {(xi, yi)}i=1,...,N,\\nwhere both xi and yi are sequences of tokens. For instance, in NL2SQL, xi\\nmight represent a natural language question and yi would be the corresponding\\nSQL query; in summarization, xi represents the article and yi would be its\\nsummary.'),\n",
       " Document(metadata={}, page_content='formance on specific tasks. Fine-tuning, on the other hand, refers to retraining\\na model pre-trained on general domains to adapt it to a particular task (Devlin\\net al., 2018; Radford et al., 2018). Some approaches only update a subset of the\\nmodel’s parameters (Collobert and Weston, 2008), but it is common practice to\\nfine-tune all parameters to achieve the best performance. However, performing')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is this paper talking about?\"\n",
    "retriever = vectordb.as_retriever()\n",
    "docs = retriever.invoke(query)\n",
    "print(f\"Number of documents retrieved: {len(docs)}\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382f34c",
   "metadata": {},
   "source": [
    "## Task 6 - Construct a QA Bot that leverages the LangChain and LLM to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model_id: str = \"tiiuae/falcon-7b-instruct\",  # puedes cambiar por otro\n",
    "            max_new_tokens: int = 512,\n",
    "            temperature: float = 0.7,\n",
    "            device: int = 0  # -1 para CPU, 0 para primera GPU\n",
    "           ):\n",
    "    # Cargar tokenizer y modelo\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "    # Crear pipeline de generación de texto\n",
    "    hf_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        return_full_text=False\n",
    "    )\n",
    "\n",
    "    # Integrar con LangChain\n",
    "    llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498aa60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5745e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4379a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How are you?\"\n",
    "\n",
    "query_result = huggingface_embedding.embed_query(query)\n",
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e695a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
